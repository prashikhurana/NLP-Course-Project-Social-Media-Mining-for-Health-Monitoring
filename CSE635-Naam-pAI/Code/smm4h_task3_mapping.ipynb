{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task3Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT2UCwkT5hW9",
        "colab_type": "text"
      },
      "source": [
        "MOUNT GOOGLE DRIVE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igN3PZVEnQhu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_list = [10000081,10000125,10000424,10000447,10000448,10000496,10001118,10001125,10001488,10001495,\n",
        " 10001502,10001540,10001598,10001639,10001718,10001720,10002199,10002368,10002855,10002942, 10003017,10003028,10003030,10003119,10003591,10003731,\n",
        " 10003738,10003743,10003785,10003978,10003988,10004063,10004203,10004716,10004969,10005103,10005124,10005264,10005265,10005613,10005645,\n",
        " 10005744,10005755,10005808,10005889,10005911,10006120,10006325,10006338,10006502,10006774,10006784,10006804,10006956,10007216,10007517,\n",
        " 10007943,10008477,10008479,10009696,10009846, 10009851, 10009866, 10010085, 10010219, 10010251, 10010300, 10010774, 10010914, 10011224,\n",
        " 10011293, 10011469, 10011865, 10011906, 10012174, 10012217, 10012239, 10012336, 10012359, 10012374, 10012378, 10012398, 10012536, 10012594,\n",
        " 10012727, 10012790, 10012792, 10012804, 10013014, 10013050, 10013082, 10013087, 10013457, 10013573, 10013580, 10013632, 10013642, 10013649,\n",
        " 10013661, 10013663, 10013710, 10013746, 10013754, 10013767, 10013778, 10013781, 10013782, 10013789, 10013932, 10014358, 10014554, 10014555,\n",
        " 10014559, 10014560, 10014843, 10015118, 10015595, 10015598, 10015667, 10016256, 10016275, 10016322, 10016323, 10016326, 10016330, 10016333,\n",
        " 10016334, 10016335, 10016336, 10016337, 10016340, 10016342, 10016344, 10016356, 10016364, 10016365, 10016366, 10016370, 10016374, 10016384,\n",
        " 10016558, 10016717, 10016759, 10016782, 10016791, 10016807, 10016821, 10016876, 10017060, 10017367, 10017375, 10017565, 10017949, 10017999,\n",
        " 10018028, 10018304, 10018730, 10018732, 10019045, 10019058, 10019062, 10019063, 10019064, 10019077, 10019136, 10019158, 10019198, 10019203,\n",
        " 10019211, 10019297, 10019300, 10019303, 10019304, 10019323, 10020197, 10020407, 10020466, 10020467, 10020554, 10020965, 10021030, 10021097,\n",
        " 10021345, 10021402, 10021417, 10021425, 10021550, 10021574, 10021654, 10021681, 10021689, 10022053, 10022086, 10022437, 10022454, 10022876,\n",
        " 10022989, 10023000, 10023089, 10023091, 10023093, 10023094, 10023163, 10023188, 10023191, 10023211, 10023222, 10023437, 10023477, 10023614,\n",
        " 10023666,10024130, 10024262, 10024490, 10024492, 10024668, 10024855, 10024862, 10024870, 10024890, 10024891, 10024895, 10024919, 10025082,\n",
        " 10026749, 10026781, 10026782, 10027174, 10027175, 10027176, 10027348, 10027352, 10027372, 10027374, 10027599, 10027951, 10028006, 10028045,\n",
        " 10028055, 10028322, 10028323, 10028333, 10028334, 10028339, 10028391, 10028771, 10028813, 10028822, 10028823, 10028831, 10028836, 10029177,\n",
        " 10029181, 10029298, 10029411, 10029412, 10029414, 10029792, 10029852, 10029855, 10029898, 10030095, 10033371, 10033421, 10033431, 10033432,\n",
        " 10033434, 10033499, 10033522, 10033532, 10033556, 10033557, 10033645, 10033664, 10033665, 10033775, 10033864, 10033987, 10034100, 10034376,\n",
        " 10034719, 10035067, 10035805, 10036155, 10036160, 10036402, 10036507, 10036596, 10036661, 10036826, 10036828, 10037234, 10037254, 10037484,\n",
        " 10037804, 10037844, 10038001, 10038194, 10038683, 10038740, 10038741, 10038742, 10038743, 10038744, 10039379, 10039626, 10039740, 10039897,\n",
        " 10039906, 10039910, 10040477, 10040528, 10040530, 10040558, 10040559, 10040604, 10040617, 10040831, 10040995, 10040999, 10041000, 10041001,\n",
        " 10041002, 10041005, 10041007, 10041012, 10041014, 10041017, 10041018, 10041049, 10041052, 10041053, 10041349, 10041374, 10041384, 10041470,\n",
        " 10041902, 10042030, 10042076, 10042112, 10042165, 10042209, 10042213, 10042457, 10042458, 10042494, 10042661, 10042693, 10042700, 10043087,\n",
        " 10043111, 10043132, 10043134, 10043169, 10043176, 10043204, 10043239, 10043242, 10043248, 10043255, 10043272, 10043436, 10043439, 10043498,\n",
        " 10043709, 10043876, 10043890, 10043900, 10043945, 10044034, 10044124, 10044126, 10044565, 10044573, 10044577, 10044698, 10045148, 10045198,\n",
        " 10045319, 10045549, 10046317, 10046543, 10046571, 10046600, 10047184, 10047340, 10047508, 10047516, 10047522, 10047545, 10047626, 10047666,\n",
        " 10047700, 10047810, 10047839, 10047862, 10047896, 10047898, 10047900, 10047904, 10048010,10048013, 10048024, 10048231, 10048232, 10048345,\n",
        " 10048469, 10048602, 10048948, 10049119, 10049183, 10049238, 10049278, 10049338, 10049475, 10049709, 10049901, 10050007, 10050218, 10051602,\n",
        " 10052066, 10052087, 10052804, 10054177, 10054248, 10056465, 10056478, 10056484, 10057071, 10057326, 10057841, 10058672, 10058683, 10058726,\n",
        " 10059133, 10059272, 10059933, 10060732, 10061087, 10061230, 10061257, 10061461, 10061758, 10061814, 10062375,10062428, 10062519, 10062948,\n",
        " 10063006, 10063053, 10064160, 10065015, 10065032, 10065702, 10065992, 10066371, 10067192, 10067568, 10067843, 10068111, 10070539, 10070679,\n",
        " 10071175, 10073281, 10074073, 10074314, 10074693, 10076326, 10076757, 10077275, 10077283, 10079024, 10079069, 10079099, 10079614]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxEmiDk6nSAm",
        "colab_type": "code",
        "outputId": "4d446ec5-929e-499d-9c74-02ca0e2acc7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "LabelToNumber = {}\n",
        "val = 0\n",
        "for i in range(len(label_list)):\n",
        "  if label_list[i] in LabelToNumber.keys() :\n",
        "    b = 1 \n",
        "  else:\n",
        "    LabelToNumber[label_list[i]]=val\n",
        "    val = val + 1\n",
        "print(len(LabelToNumber.keys()))\n",
        "print(LabelToNumber)\n",
        "\n",
        "\n",
        "NumberToLabel = {}\n",
        "val = 0\n",
        "for key in LabelToNumber.keys():\n",
        "  NumberToLabel[val] = key\n",
        "  val = val + 1\n",
        "print(len(NumberToLabel.keys()))\n",
        "print(NumberToLabel)\n",
        "\n",
        "def numbertolabel(data):\n",
        "  return NumberToLabel[data]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "475\n",
            "{10000081: 0, 10000125: 1, 10000424: 2, 10000447: 3, 10000448: 4, 10000496: 5, 10001118: 6, 10001125: 7, 10001488: 8, 10001495: 9, 10001502: 10, 10001540: 11, 10001598: 12, 10001639: 13, 10001718: 14, 10001720: 15, 10002199: 16, 10002368: 17, 10002855: 18, 10002942: 19, 10003017: 20, 10003028: 21, 10003030: 22, 10003119: 23, 10003591: 24, 10003731: 25, 10003738: 26, 10003743: 27, 10003785: 28, 10003978: 29, 10003988: 30, 10004063: 31, 10004203: 32, 10004716: 33, 10004969: 34, 10005103: 35, 10005124: 36, 10005264: 37, 10005265: 38, 10005613: 39, 10005645: 40, 10005744: 41, 10005755: 42, 10005808: 43, 10005889: 44, 10005911: 45, 10006120: 46, 10006325: 47, 10006338: 48, 10006502: 49, 10006774: 50, 10006784: 51, 10006804: 52, 10006956: 53, 10007216: 54, 10007517: 55, 10007943: 56, 10008477: 57, 10008479: 58, 10009696: 59, 10009846: 60, 10009851: 61, 10009866: 62, 10010085: 63, 10010219: 64, 10010251: 65, 10010300: 66, 10010774: 67, 10010914: 68, 10011224: 69, 10011293: 70, 10011469: 71, 10011865: 72, 10011906: 73, 10012174: 74, 10012217: 75, 10012239: 76, 10012336: 77, 10012359: 78, 10012374: 79, 10012378: 80, 10012398: 81, 10012536: 82, 10012594: 83, 10012727: 84, 10012790: 85, 10012792: 86, 10012804: 87, 10013014: 88, 10013050: 89, 10013082: 90, 10013087: 91, 10013457: 92, 10013573: 93, 10013580: 94, 10013632: 95, 10013642: 96, 10013649: 97, 10013661: 98, 10013663: 99, 10013710: 100, 10013746: 101, 10013754: 102, 10013767: 103, 10013778: 104, 10013781: 105, 10013782: 106, 10013789: 107, 10013932: 108, 10014358: 109, 10014554: 110, 10014555: 111, 10014559: 112, 10014560: 113, 10014843: 114, 10015118: 115, 10015595: 116, 10015598: 117, 10015667: 118, 10016256: 119, 10016275: 120, 10016322: 121, 10016323: 122, 10016326: 123, 10016330: 124, 10016333: 125, 10016334: 126, 10016335: 127, 10016336: 128, 10016337: 129, 10016340: 130, 10016342: 131, 10016344: 132, 10016356: 133, 10016364: 134, 10016365: 135, 10016366: 136, 10016370: 137, 10016374: 138, 10016384: 139, 10016558: 140, 10016717: 141, 10016759: 142, 10016782: 143, 10016791: 144, 10016807: 145, 10016821: 146, 10016876: 147, 10017060: 148, 10017367: 149, 10017375: 150, 10017565: 151, 10017949: 152, 10017999: 153, 10018028: 154, 10018304: 155, 10018730: 156, 10018732: 157, 10019045: 158, 10019058: 159, 10019062: 160, 10019063: 161, 10019064: 162, 10019077: 163, 10019136: 164, 10019158: 165, 10019198: 166, 10019203: 167, 10019211: 168, 10019297: 169, 10019300: 170, 10019303: 171, 10019304: 172, 10019323: 173, 10020197: 174, 10020407: 175, 10020466: 176, 10020467: 177, 10020554: 178, 10020965: 179, 10021030: 180, 10021097: 181, 10021345: 182, 10021402: 183, 10021417: 184, 10021425: 185, 10021550: 186, 10021574: 187, 10021654: 188, 10021681: 189, 10021689: 190, 10022053: 191, 10022086: 192, 10022437: 193, 10022454: 194, 10022876: 195, 10022989: 196, 10023000: 197, 10023089: 198, 10023091: 199, 10023093: 200, 10023094: 201, 10023163: 202, 10023188: 203, 10023191: 204, 10023211: 205, 10023222: 206, 10023437: 207, 10023477: 208, 10023614: 209, 10023666: 210, 10024130: 211, 10024262: 212, 10024490: 213, 10024492: 214, 10024668: 215, 10024855: 216, 10024862: 217, 10024870: 218, 10024890: 219, 10024891: 220, 10024895: 221, 10024919: 222, 10025082: 223, 10026749: 224, 10026781: 225, 10026782: 226, 10027174: 227, 10027175: 228, 10027176: 229, 10027348: 230, 10027352: 231, 10027372: 232, 10027374: 233, 10027599: 234, 10027951: 235, 10028006: 236, 10028045: 237, 10028055: 238, 10028322: 239, 10028323: 240, 10028333: 241, 10028334: 242, 10028339: 243, 10028391: 244, 10028771: 245, 10028813: 246, 10028822: 247, 10028823: 248, 10028831: 249, 10028836: 250, 10029177: 251, 10029181: 252, 10029298: 253, 10029411: 254, 10029412: 255, 10029414: 256, 10029792: 257, 10029852: 258, 10029855: 259, 10029898: 260, 10030095: 261, 10033371: 262, 10033421: 263, 10033431: 264, 10033432: 265, 10033434: 266, 10033499: 267, 10033522: 268, 10033532: 269, 10033556: 270, 10033557: 271, 10033645: 272, 10033664: 273, 10033665: 274, 10033775: 275, 10033864: 276, 10033987: 277, 10034100: 278, 10034376: 279, 10034719: 280, 10035067: 281, 10035805: 282, 10036155: 283, 10036160: 284, 10036402: 285, 10036507: 286, 10036596: 287, 10036661: 288, 10036826: 289, 10036828: 290, 10037234: 291, 10037254: 292, 10037484: 293, 10037804: 294, 10037844: 295, 10038001: 296, 10038194: 297, 10038683: 298, 10038740: 299, 10038741: 300, 10038742: 301, 10038743: 302, 10038744: 303, 10039379: 304, 10039626: 305, 10039740: 306, 10039897: 307, 10039906: 308, 10039910: 309, 10040477: 310, 10040528: 311, 10040530: 312, 10040558: 313, 10040559: 314, 10040604: 315, 10040617: 316, 10040831: 317, 10040995: 318, 10040999: 319, 10041000: 320, 10041001: 321, 10041002: 322, 10041005: 323, 10041007: 324, 10041012: 325, 10041014: 326, 10041017: 327, 10041018: 328, 10041049: 329, 10041052: 330, 10041053: 331, 10041349: 332, 10041374: 333, 10041384: 334, 10041470: 335, 10041902: 336, 10042030: 337, 10042076: 338, 10042112: 339, 10042165: 340, 10042209: 341, 10042213: 342, 10042457: 343, 10042458: 344, 10042494: 345, 10042661: 346, 10042693: 347, 10042700: 348, 10043087: 349, 10043111: 350, 10043132: 351, 10043134: 352, 10043169: 353, 10043176: 354, 10043204: 355, 10043239: 356, 10043242: 357, 10043248: 358, 10043255: 359, 10043272: 360, 10043436: 361, 10043439: 362, 10043498: 363, 10043709: 364, 10043876: 365, 10043890: 366, 10043900: 367, 10043945: 368, 10044034: 369, 10044124: 370, 10044126: 371, 10044565: 372, 10044573: 373, 10044577: 374, 10044698: 375, 10045148: 376, 10045198: 377, 10045319: 378, 10045549: 379, 10046317: 380, 10046543: 381, 10046571: 382, 10046600: 383, 10047184: 384, 10047340: 385, 10047508: 386, 10047516: 387, 10047522: 388, 10047545: 389, 10047626: 390, 10047666: 391, 10047700: 392, 10047810: 393, 10047839: 394, 10047862: 395, 10047896: 396, 10047898: 397, 10047900: 398, 10047904: 399, 10048010: 400, 10048013: 401, 10048024: 402, 10048231: 403, 10048232: 404, 10048345: 405, 10048469: 406, 10048602: 407, 10048948: 408, 10049119: 409, 10049183: 410, 10049238: 411, 10049278: 412, 10049338: 413, 10049475: 414, 10049709: 415, 10049901: 416, 10050007: 417, 10050218: 418, 10051602: 419, 10052066: 420, 10052087: 421, 10052804: 422, 10054177: 423, 10054248: 424, 10056465: 425, 10056478: 426, 10056484: 427, 10057071: 428, 10057326: 429, 10057841: 430, 10058672: 431, 10058683: 432, 10058726: 433, 10059133: 434, 10059272: 435, 10059933: 436, 10060732: 437, 10061087: 438, 10061230: 439, 10061257: 440, 10061461: 441, 10061758: 442, 10061814: 443, 10062375: 444, 10062428: 445, 10062519: 446, 10062948: 447, 10063006: 448, 10063053: 449, 10064160: 450, 10065015: 451, 10065032: 452, 10065702: 453, 10065992: 454, 10066371: 455, 10067192: 456, 10067568: 457, 10067843: 458, 10068111: 459, 10070539: 460, 10070679: 461, 10071175: 462, 10073281: 463, 10074073: 464, 10074314: 465, 10074693: 466, 10076326: 467, 10076757: 468, 10077275: 469, 10077283: 470, 10079024: 471, 10079069: 472, 10079099: 473, 10079614: 474}\n",
            "475\n",
            "{0: 10000081, 1: 10000125, 2: 10000424, 3: 10000447, 4: 10000448, 5: 10000496, 6: 10001118, 7: 10001125, 8: 10001488, 9: 10001495, 10: 10001502, 11: 10001540, 12: 10001598, 13: 10001639, 14: 10001718, 15: 10001720, 16: 10002199, 17: 10002368, 18: 10002855, 19: 10002942, 20: 10003017, 21: 10003028, 22: 10003030, 23: 10003119, 24: 10003591, 25: 10003731, 26: 10003738, 27: 10003743, 28: 10003785, 29: 10003978, 30: 10003988, 31: 10004063, 32: 10004203, 33: 10004716, 34: 10004969, 35: 10005103, 36: 10005124, 37: 10005264, 38: 10005265, 39: 10005613, 40: 10005645, 41: 10005744, 42: 10005755, 43: 10005808, 44: 10005889, 45: 10005911, 46: 10006120, 47: 10006325, 48: 10006338, 49: 10006502, 50: 10006774, 51: 10006784, 52: 10006804, 53: 10006956, 54: 10007216, 55: 10007517, 56: 10007943, 57: 10008477, 58: 10008479, 59: 10009696, 60: 10009846, 61: 10009851, 62: 10009866, 63: 10010085, 64: 10010219, 65: 10010251, 66: 10010300, 67: 10010774, 68: 10010914, 69: 10011224, 70: 10011293, 71: 10011469, 72: 10011865, 73: 10011906, 74: 10012174, 75: 10012217, 76: 10012239, 77: 10012336, 78: 10012359, 79: 10012374, 80: 10012378, 81: 10012398, 82: 10012536, 83: 10012594, 84: 10012727, 85: 10012790, 86: 10012792, 87: 10012804, 88: 10013014, 89: 10013050, 90: 10013082, 91: 10013087, 92: 10013457, 93: 10013573, 94: 10013580, 95: 10013632, 96: 10013642, 97: 10013649, 98: 10013661, 99: 10013663, 100: 10013710, 101: 10013746, 102: 10013754, 103: 10013767, 104: 10013778, 105: 10013781, 106: 10013782, 107: 10013789, 108: 10013932, 109: 10014358, 110: 10014554, 111: 10014555, 112: 10014559, 113: 10014560, 114: 10014843, 115: 10015118, 116: 10015595, 117: 10015598, 118: 10015667, 119: 10016256, 120: 10016275, 121: 10016322, 122: 10016323, 123: 10016326, 124: 10016330, 125: 10016333, 126: 10016334, 127: 10016335, 128: 10016336, 129: 10016337, 130: 10016340, 131: 10016342, 132: 10016344, 133: 10016356, 134: 10016364, 135: 10016365, 136: 10016366, 137: 10016370, 138: 10016374, 139: 10016384, 140: 10016558, 141: 10016717, 142: 10016759, 143: 10016782, 144: 10016791, 145: 10016807, 146: 10016821, 147: 10016876, 148: 10017060, 149: 10017367, 150: 10017375, 151: 10017565, 152: 10017949, 153: 10017999, 154: 10018028, 155: 10018304, 156: 10018730, 157: 10018732, 158: 10019045, 159: 10019058, 160: 10019062, 161: 10019063, 162: 10019064, 163: 10019077, 164: 10019136, 165: 10019158, 166: 10019198, 167: 10019203, 168: 10019211, 169: 10019297, 170: 10019300, 171: 10019303, 172: 10019304, 173: 10019323, 174: 10020197, 175: 10020407, 176: 10020466, 177: 10020467, 178: 10020554, 179: 10020965, 180: 10021030, 181: 10021097, 182: 10021345, 183: 10021402, 184: 10021417, 185: 10021425, 186: 10021550, 187: 10021574, 188: 10021654, 189: 10021681, 190: 10021689, 191: 10022053, 192: 10022086, 193: 10022437, 194: 10022454, 195: 10022876, 196: 10022989, 197: 10023000, 198: 10023089, 199: 10023091, 200: 10023093, 201: 10023094, 202: 10023163, 203: 10023188, 204: 10023191, 205: 10023211, 206: 10023222, 207: 10023437, 208: 10023477, 209: 10023614, 210: 10023666, 211: 10024130, 212: 10024262, 213: 10024490, 214: 10024492, 215: 10024668, 216: 10024855, 217: 10024862, 218: 10024870, 219: 10024890, 220: 10024891, 221: 10024895, 222: 10024919, 223: 10025082, 224: 10026749, 225: 10026781, 226: 10026782, 227: 10027174, 228: 10027175, 229: 10027176, 230: 10027348, 231: 10027352, 232: 10027372, 233: 10027374, 234: 10027599, 235: 10027951, 236: 10028006, 237: 10028045, 238: 10028055, 239: 10028322, 240: 10028323, 241: 10028333, 242: 10028334, 243: 10028339, 244: 10028391, 245: 10028771, 246: 10028813, 247: 10028822, 248: 10028823, 249: 10028831, 250: 10028836, 251: 10029177, 252: 10029181, 253: 10029298, 254: 10029411, 255: 10029412, 256: 10029414, 257: 10029792, 258: 10029852, 259: 10029855, 260: 10029898, 261: 10030095, 262: 10033371, 263: 10033421, 264: 10033431, 265: 10033432, 266: 10033434, 267: 10033499, 268: 10033522, 269: 10033532, 270: 10033556, 271: 10033557, 272: 10033645, 273: 10033664, 274: 10033665, 275: 10033775, 276: 10033864, 277: 10033987, 278: 10034100, 279: 10034376, 280: 10034719, 281: 10035067, 282: 10035805, 283: 10036155, 284: 10036160, 285: 10036402, 286: 10036507, 287: 10036596, 288: 10036661, 289: 10036826, 290: 10036828, 291: 10037234, 292: 10037254, 293: 10037484, 294: 10037804, 295: 10037844, 296: 10038001, 297: 10038194, 298: 10038683, 299: 10038740, 300: 10038741, 301: 10038742, 302: 10038743, 303: 10038744, 304: 10039379, 305: 10039626, 306: 10039740, 307: 10039897, 308: 10039906, 309: 10039910, 310: 10040477, 311: 10040528, 312: 10040530, 313: 10040558, 314: 10040559, 315: 10040604, 316: 10040617, 317: 10040831, 318: 10040995, 319: 10040999, 320: 10041000, 321: 10041001, 322: 10041002, 323: 10041005, 324: 10041007, 325: 10041012, 326: 10041014, 327: 10041017, 328: 10041018, 329: 10041049, 330: 10041052, 331: 10041053, 332: 10041349, 333: 10041374, 334: 10041384, 335: 10041470, 336: 10041902, 337: 10042030, 338: 10042076, 339: 10042112, 340: 10042165, 341: 10042209, 342: 10042213, 343: 10042457, 344: 10042458, 345: 10042494, 346: 10042661, 347: 10042693, 348: 10042700, 349: 10043087, 350: 10043111, 351: 10043132, 352: 10043134, 353: 10043169, 354: 10043176, 355: 10043204, 356: 10043239, 357: 10043242, 358: 10043248, 359: 10043255, 360: 10043272, 361: 10043436, 362: 10043439, 363: 10043498, 364: 10043709, 365: 10043876, 366: 10043890, 367: 10043900, 368: 10043945, 369: 10044034, 370: 10044124, 371: 10044126, 372: 10044565, 373: 10044573, 374: 10044577, 375: 10044698, 376: 10045148, 377: 10045198, 378: 10045319, 379: 10045549, 380: 10046317, 381: 10046543, 382: 10046571, 383: 10046600, 384: 10047184, 385: 10047340, 386: 10047508, 387: 10047516, 388: 10047522, 389: 10047545, 390: 10047626, 391: 10047666, 392: 10047700, 393: 10047810, 394: 10047839, 395: 10047862, 396: 10047896, 397: 10047898, 398: 10047900, 399: 10047904, 400: 10048010, 401: 10048013, 402: 10048024, 403: 10048231, 404: 10048232, 405: 10048345, 406: 10048469, 407: 10048602, 408: 10048948, 409: 10049119, 410: 10049183, 411: 10049238, 412: 10049278, 413: 10049338, 414: 10049475, 415: 10049709, 416: 10049901, 417: 10050007, 418: 10050218, 419: 10051602, 420: 10052066, 421: 10052087, 422: 10052804, 423: 10054177, 424: 10054248, 425: 10056465, 426: 10056478, 427: 10056484, 428: 10057071, 429: 10057326, 430: 10057841, 431: 10058672, 432: 10058683, 433: 10058726, 434: 10059133, 435: 10059272, 436: 10059933, 437: 10060732, 438: 10061087, 439: 10061230, 440: 10061257, 441: 10061461, 442: 10061758, 443: 10061814, 444: 10062375, 445: 10062428, 446: 10062519, 447: 10062948, 448: 10063006, 449: 10063053, 450: 10064160, 451: 10065015, 452: 10065032, 453: 10065702, 454: 10065992, 455: 10066371, 456: 10067192, 457: 10067568, 458: 10067843, 459: 10068111, 460: 10070539, 461: 10070679, 462: 10071175, 463: 10073281, 464: 10074073, 465: 10074314, 466: 10074693, 467: 10076326, 468: 10076757, 469: 10077275, 470: 10077283, 471: 10079024, 472: 10079069, 473: 10079099, 474: 10079614}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4--Hj6_5kxm",
        "colab_type": "code",
        "outputId": "5466d5eb-480d-4467-abff-0b2a581ed1f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5neDONOi9TyX",
        "colab_type": "text"
      },
      "source": [
        "INSTALL REQUIREMENTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rSJ6FDh57mS",
        "colab_type": "code",
        "outputId": "40e2faed-137b-4269-c79a-3f2ff836eb42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install tweet-preprocessor\n",
        "!pip install transformers\n",
        "!pip install emosent-py\n",
        "!pip install ekphrasis\n",
        "!pip install demoji\n",
        "!pip install sister\n",
        "import sister\n",
        "embedder = sister.MeanEmbedding(lang=\"en\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tweet-preprocessor\n",
            "  Downloading https://files.pythonhosted.org/packages/2a/f8/810ec35c31cca89bc4f1a02c14b042b9ec6c19dd21f7ef1876874ef069a6/tweet-preprocessor-0.5.0.tar.gz\n",
            "Building wheels for collected packages: tweet-preprocessor\n",
            "  Building wheel for tweet-preprocessor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tweet-preprocessor: filename=tweet_preprocessor-0.5.0-cp36-none-any.whl size=7947 sha256=91476b69fcf126d17d0baa77faa64119aa7756a8539f1716f58f6407a265c23e\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/27/cc/49938e98a2470802ebdefae9d2b3f524768e970c1ebbe2dc4a\n",
            "Successfully built tweet-preprocessor\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.5.0\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\u001b[K     |████████████████████████████████| 573kB 2.8MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 44.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 40.9MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 37.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.47)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.47)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=f55556c7cbeaad51a7dd60ca9027a7d1f2abf2bb545dbdf9e448dfd66e5be0ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.86 tokenizers-0.5.2 transformers-2.8.0\n",
            "Collecting emosent-py\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d7/00c5117b7f81c53d764520e0a5032e678f1b3e54daee60308a9bfe6a94d9/emosent-py-0.1.6.tar.gz\n",
            "Building wheels for collected packages: emosent-py\n",
            "  Building wheel for emosent-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emosent-py: filename=emosent_py-0.1.6-cp36-none-any.whl size=28504 sha256=3e385d7bd18876637b0bbaf8ba7d8487c6fd391aa89e7f3dadaa7fcccd44bb76\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/32/3c/2e21c3622b77cdc89a38a711240588ac3cf9b8e805eed0f6e1\n",
            "Successfully built emosent-py\n",
            "Installing collected packages: emosent-py\n",
            "Successfully installed emosent-py-0.1.6\n",
            "Collecting ekphrasis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/e6/37c59d65e78c3a2aaf662df58faca7250eb6b36c559b912a39a7ca204cfb/ekphrasis-0.5.1.tar.gz (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 2.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (4.38.0)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Collecting ujson\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/e4/a79c57e22d6d09bbeb5e8febb8cfa0fe10ede69eed9c3458d3ec99014e20/ujson-2.0.3-cp36-cp36m-manylinux1_x86_64.whl (174kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (3.2.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (3.2.5)\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/d8/5e877ac5e827eaa41a7ea8c0dc1d3042e05d7e337604dc2aedb854e7b500/ftfy-5.7.tar.gz (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (1.18.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->ekphrasis) (1.12.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->ekphrasis) (0.1.9)\n",
            "Building wheels for collected packages: ekphrasis, ftfy\n",
            "  Building wheel for ekphrasis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ekphrasis: filename=ekphrasis-0.5.1-cp36-none-any.whl size=82843 sha256=de419b62ee1ea1c4478f9c272ed5c57af2c708b4b84b3054563d1508efe96846\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/c5/9b/c9b60f535a2cf9fdbc92d84c4801a010c35a9cd348011ed2a1\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.7-cp36-none-any.whl size=44593 sha256=c2e30079e301244630635c89dd3c22965ca2318bd8e8a4c0c1e38766bd9c7908\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/da/59/6c8925d571aacade638a0f515960c21c0887af1bfe31908fbf\n",
            "Successfully built ekphrasis ftfy\n",
            "Installing collected packages: colorama, ujson, ftfy, ekphrasis\n",
            "Successfully installed colorama-0.4.3 ekphrasis-0.5.1 ftfy-5.7 ujson-2.0.3\n",
            "Collecting demoji\n",
            "  Downloading https://files.pythonhosted.org/packages/da/0b/d008f26ebbfd86d21117267e627f2f7359c76e5ecbeba08d8f631f4092c4/demoji-0.2.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from demoji) (46.1.3)\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.6/dist-packages (from demoji) (2.23.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from demoji) (0.4.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (3.0.4)\n",
            "Installing collected packages: demoji\n",
            "Successfully installed demoji-0.2.1\n",
            "Collecting sister\n",
            "  Downloading https://files.pythonhosted.org/packages/b8/84/6ff468f37bd925df22bb6917951fc804133621c19083ae8a53c61023f236/sister-0.1.7.tar.gz\n",
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sister) (1.18.3)\n",
            "Collecting Janome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/f0/bd7f90806132d7d9d642d418bdc3e870cfdff5947254ea3cab27480983a7/Janome-0.3.10-py2.py3-none-any.whl (21.5MB)\n",
            "\u001b[K     |████████████████████████████████| 21.5MB 1.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from sister) (3.6.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sister) (0.14.1)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext->sister) (2.5.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext->sister) (46.1.3)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim->sister) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim->sister) (1.12.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->sister) (1.11.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->sister) (1.12.47)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->sister) (2.49.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->sister) (2.23.0)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.47 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->sister) (1.15.47)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->sister) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->sister) (0.9.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->sister) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->sister) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->sister) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim->sister) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->smart-open>=1.2.1->gensim->sister) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->smart-open>=1.2.1->gensim->sister) (0.15.2)\n",
            "Building wheels for collected packages: sister, fasttext\n",
            "  Building wheel for sister (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sister: filename=sister-0.1.7-cp36-none-any.whl size=4621 sha256=9836c1becb5f556cd59fe2220540211f84c033795afbd73078cd29861338eff3\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/64/af/ac710ce836d62df663d6219586d58c57b27c0ec000db1b0aef\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3021219 sha256=b9d32a4bb3552c2b116759598e0451453ddd7d947b93a6de2085411fbc7cde33\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n",
            "Successfully built sister fasttext\n",
            "Installing collected packages: fasttext, Janome, sister\n",
            "Successfully installed Janome-0.3.10 fasttext-0.9.2 sister-0.1.7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading from https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.simple.zip...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2Xxts9G6phw",
        "colab_type": "text"
      },
      "source": [
        "SOME GLOBAL VARIABLES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "du1XCMkp6I4A",
        "colab_type": "code",
        "outputId": "3ce1d6f0-09da-4325-e9db-91a15f58a7ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import re\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import nltk \n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.util import ngrams\n",
        "from nltk.corpus import names\n",
        "nltk.download('names')\n",
        "\n",
        "from gensim.parsing.preprocessing import strip_punctuation\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "import string"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/names.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxnAkSjn6rmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "medexp = {}\n",
        "medexpugt = {}\n",
        "entiremed = {}\n",
        "medexptweets = {}\n",
        "ftembed = 300\n",
        "medsent = np.zeros((1, ftembed))\n",
        "medtest = []\n",
        "medcode = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc_hsirZ6khm",
        "colab_type": "text"
      },
      "source": [
        "LOAD DATA INTO DATAFRAMES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBjIUpbd6m00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "def getUGTExpandedSpecific():\n",
        "  global entiremed\n",
        "  entiremed = json.load(open('./drive/My Drive/DataFolder/entiremedcorpus.json', 'r'))\n",
        "getUGTExpandedSpecific()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXGH280e7YVA",
        "colab_type": "text"
      },
      "source": [
        "GET DATA INTO DICTIONARY - CODE TO TERM AND TERM TO CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iX9haD547ccP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "med = pd.read_excel(r'./drive/My Drive/DataFolder/MEDFINALUSETHIS - R1.xlsx')\n",
        "med = med.dropna(axis='columns', how='all')\n",
        "medc = list(med['meddra_code'])\n",
        "medt = list(med['meddra_term'])\n",
        "medCodeToTerm = {}\n",
        "medTermToCode = {}\n",
        "\n",
        "for i in range(len(medc)):\n",
        "  x = str(medc[i])\n",
        "  medCodeToTerm[x] = medt[i]\n",
        "\n",
        "for i in range(len(medt)):\n",
        "  x = str(medt[i])\n",
        "  medTermToCode[x] = medc[i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XggDdqX65Yr3",
        "colab_type": "text"
      },
      "source": [
        "DATA CLEANING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmaFNS7B5VK2",
        "colab_type": "code",
        "outputId": "b63587fb-8277-4ab7-af5e-7f56e8e92a00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "import tweepy\n",
        "import csv\n",
        "import json\n",
        "import preprocessor as p\n",
        "import re\n",
        "import demoji\n",
        "from emosent import get_emoji_sentiment_rank\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.classes.segmenter import Segmenter\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk import word_tokenize\n",
        "import re\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    normalize=['email', 'money', 'phone',\n",
        "        'time', 'date'],\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'},\n",
        "    fix_html=True,\n",
        "    segmenter=\"twitter\",\n",
        "    corrector=\"twitter\",    \n",
        "    unpack_hashtags=True,\n",
        "    unpack_contractions=True,\n",
        "    spell_correct_elong=True,\n",
        "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
        "    dicts=[emoticons],\n",
        "    remove_tags=True\n",
        ")\n",
        "to_remove = ['<hashtag>', '</hashtag>', '<allcaps>', '</allcaps>', '<elongated>', '</elongated>',\n",
        "            '<repeated>', '</repeated>', '<emphasis>', '</emphasis>', '<censored>', '</censored>']\n",
        "\n",
        "def basic_cleaning(tweet):\n",
        "    p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.SMILEY, p.OPT.MENTION, p.OPT.RESERVED)\n",
        "    tweet = p.clean(tweet)\n",
        "    tweet = tweet.encode('ascii','ignore').decode()\n",
        "    return tweet.encode('ascii','ignore').decode()\n",
        "\n",
        "def using_ekphrasis(tweet):\n",
        "    txt = \" \".join(text_processor.pre_process_doc(tweet))\n",
        "    for rem in to_remove:\n",
        "        if rem in txt:\n",
        "            txt = txt.replace(rem, \"\")\n",
        "    txt = re.sub('<|>','',txt)\n",
        "    txt = re.sub(' +',' ',txt).strip()\n",
        "    return txt\n",
        "\n",
        "def stemmer(word):\n",
        "    #print(\"PORTER STEMMER\")\n",
        "    word = word.lower()\n",
        "    word = word_tokenize(word)\n",
        "    ps = PorterStemmer() \n",
        "    filtered_sentence = \"\"\n",
        "    for w in word: \n",
        "        filtered_sentence = filtered_sentence +\" \"+ ps.stem(w)\n",
        "    return filtered_sentence\n",
        "\n",
        "def preprocess(tweet):\n",
        "  if isNotNan(tweet):\n",
        "    tweet = tweet.lower()\n",
        "    tweet = using_ekphrasis(tweet)\n",
        "    tweet = basic_cleaning(tweet)\n",
        "    tweet = re.sub(r'[^a-zA-Z0-9,;!? ]','',tweet)\n",
        "    tweet = re.sub(' +',' ',tweet).strip()\n",
        "    #tweet = stemmer(tweet)\n",
        "    return tweet\n",
        "  else:\n",
        "    return tweet\n",
        "\n",
        "def preprecossStem(tweet):\n",
        "    tweet = tweet.lower()\n",
        "    tweet = using_ekphrasis(tweet)\n",
        "    tweet = basic_cleaning(tweet)\n",
        "    tweet = re.sub(r'[^a-zA-Z0-9,;!? ]','',tweet)\n",
        "    tweet = re.sub(' +',' ',tweet).strip()\n",
        "    tweet = stemmer(tweet)\n",
        "    return tweet"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Word statistics files not found!\n",
            "Downloading... done!\n",
            "Unpacking... done!\n",
            "Reading twitter - 1grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_1grams.txt\n",
            "Reading twitter - 2grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_2grams.txt\n",
            "Reading twitter - 1grams ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4RxasYx6ArA",
        "colab_type": "text"
      },
      "source": [
        "UTILITY FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0o5K3306BDT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def isNotNan(string):\n",
        "    if (isNaN(string)):\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "\n",
        "def isNaN(string):\n",
        "    return string != string\n",
        "\n",
        "def cleannumber(text):\n",
        "  if isNotNan(text):\n",
        "    return int(text)\n",
        "\n",
        "\n",
        "import random\n",
        "def getRandomSample(l1):\n",
        "  minsize = min(len(l1),60)\n",
        "  return random.sample(l1, minsize)\n",
        "\n",
        "def medicaldicsent(l1,factor):\n",
        "  wordembed = np.zeros((1, ftembed))\n",
        "  if len(l1)==1:\n",
        "    onew = l1[0]\n",
        "    #print(onew)\n",
        "    if isNotNan(onew) and onew!='' and onew!=\" \":\n",
        "      onew = preprocess(onew)\n",
        "      #print(onew)\n",
        "      if (onew!='' and onew!=\" \" and isNotNan(onew)):\n",
        "          wordembed = np.vstack([wordembed, embedder(onew)])\n",
        "  else:\n",
        "    for i in range(1,len(l1)):\n",
        "      onew = l1[i]\n",
        "      #print(onew)\n",
        "      if isNotNan(onew) and onew!='' and onew!=\" \":\n",
        "        onew = preprocess(onew)\n",
        "        #print(onew)\n",
        "        if (onew!='' and onew!=\" \" and isNotNan(onew)):\n",
        "            wordembed = np.vstack([wordembed, embedder(onew)])\n",
        "  wordembed = float(factor)*np.array(wordembed)\n",
        "  wordembed = np.delete(wordembed, 0, 0)\n",
        "  wordembed = np.mean(wordembed, axis=0)\n",
        "  wordembed = np.vstack([wordembed, embedder(l1[0])])\n",
        "  wordembed = np.mean(wordembed, axis=0)\n",
        "  wordembed = np.reshape((wordembed), (1, ftembed))\n",
        "  return wordembed\n",
        "\n",
        "def getembeddingssentence(v):\n",
        "      v = v.lower()\n",
        "      v = preprecossStem(v)\n",
        "      se = np.reshape((embedder(v)), (1, ftembed))  \n",
        "      return se\n",
        "\n",
        "def getembeddingssentencex(getextract,tweet,factor):\n",
        "  getextract = (getextract.lower())\n",
        "  tweet = preprecossStem(tweet.lower())\n",
        "  wordembed = np.zeros((1, ftembed))\n",
        "  se = np.reshape((embedder(getextract)), (1, ftembed))\n",
        "  wordembed = np.vstack([wordembed, se])\n",
        "  se = float(factor)*np.reshape((embedder(tweet)), (1, ftembed))\n",
        "  wordembed = np.vstack([wordembed, se])\n",
        "  wordembed = np.delete(wordembed, 0, 0)\n",
        "  wordembed = np.mean(wordembed, axis=0)\n",
        "  wordembed = np.reshape((wordembed), (1, ftembed))\n",
        "  return wordembed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhFLyQnU7n-E",
        "colab_type": "text"
      },
      "source": [
        "GET THE TERMS FOR EACH MEDDRA CLASS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hos7zMPR706g",
        "colab_type": "code",
        "outputId": "62660168-c627-4ea3-c4d4-428d3ba990e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pandas as pd\n",
        "medicalterms = pd.read_csv(\"./drive/My Drive/DataFolder/TrainingExpandedSample.csv\")\n",
        "label_list = list(medicalterms.label.unique())\n",
        "print(\"There are \",len(label_list),\" classes\");"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are  475  classes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTaKPqvk8iJE",
        "colab_type": "text"
      },
      "source": [
        "SET THE MEDICAL DICTIONARY \n",
        "And check F score on Validation Set, while adding the column to validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhnObqVQ8klV",
        "colab_type": "code",
        "outputId": "5deda65f-5c61-4cc6-bc5e-ea2904ad2146",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "alpha = [0.9]\n",
        "#med = pd.read_excel(r'./drive/My Drive/DataFolder/MEDFINALUSETHISR1.xlsx')\n",
        "med = med.dropna(axis='columns', how='all')\n",
        "medtest = list(med['meddra_term'])\n",
        "medcode = list(med['meddra_code'])\n",
        "maxscore = 0\n",
        "alphaval = 0\n",
        "betaval = 0\n",
        "task3mix = pd.DataFrame(columns=['text','label','tweet'])\n",
        "\n",
        "\n",
        "#print(\"entiremed\")\n",
        "print(\"Get the medical embeddings\")\n",
        "for j in range(len(alpha)):\n",
        "  factor1 = float(alpha[j])\n",
        "  #print(\"Get entire medical Embeddings for Factor of medicaemb:\",factor1)\n",
        "  founddic = 0\n",
        "  global medsent\n",
        "  medsent = np.zeros((1, ftembed))\n",
        "  for i in range(len(medtest)):\n",
        "      v = medtest[i]\n",
        "      a = int(medcode[i])\n",
        "      if isNotNan(v) and a in label_list:\n",
        "        #print(v)\n",
        "        key = str(med.iloc[i,1])\n",
        "        if  key in entiremed.keys():\n",
        "          se1 = medicaldicsent(entiremed[key],factor1)\n",
        "          medsent = np.vstack([medsent, se1])\n",
        "          founddic = founddic+1\n",
        "        else:\n",
        "          se1 = getembeddingssentence(v)\n",
        "          medsent = np.vstack([medsent, se1])\n",
        "  medsent = np.delete(medsent, 0, 0)\n",
        "  print(\"SANITY CHECK\",np.isnan(np.sum(medsent)))\n",
        "  \n",
        "  \n",
        "  a =  [1]\n",
        "  for t in range(len(a)):\n",
        "    factor = float(a[t])\n",
        "    #print(\"Get Train Set\")\n",
        "    testdata1 = pd.read_csv(r'./drive/My Drive/DataFolder/task3_validation.tsv', sep='\\t')\n",
        "    #testdata1 = pd.read_csv(r'task3validation.csv')\n",
        "    testdata = testdata1.dropna(axis='columns', how='all')\n",
        "    meddracode = []\n",
        "    meddraterm = []\n",
        "    originalmeddracode = []\n",
        "    for index, row in testdata.iterrows():\n",
        "        getextract = row['extraction']\n",
        "        tweet = row['tweet']\n",
        "        if (isNotNan(getextract) and getextract!=''):\n",
        "          x = getembeddingssentencex(getextract,tweet,factor)\n",
        "          b = cosine_similarity(x,medsent)\n",
        "          maxval = np.argmax(b)\n",
        "          mc = int(medcode[int(maxval)])\n",
        "          mt = medtest[int(maxval)]\n",
        "          meddracode.append(int(mc))\n",
        "          meddraterm.append(mt)\n",
        "          originalmeddracode.append(int(row['meddra_code']))\n",
        "          df2 = pd.DataFrame({\"text\":getextract,\"label\":int(row['meddra_code']),\"tweet\":tweet},index=[0]) \n",
        "          task3mix = task3mix.append(df2, ignore_index = True)\n",
        "      \n",
        "    #print(\"Get accuracy score\")\n",
        "    #print(\"len of original\",len(originalmeddracode))\n",
        "    #print(\"len of predicted\",len(meddracode))\n",
        "    #print(\"     Factor of tweet:\",factor)\n",
        "    fs = f1_score(originalmeddracode, meddracode, average='macro')\n",
        "    acc = accuracy_score(originalmeddracode, meddracode)\n",
        "    if fs > maxscore:\n",
        "      maxscore = fs\n",
        "      alphaval = factor1\n",
        "      betaval = factor \n",
        "    print(\" Medical Sent:\",factor1,\" Tweet Factor\",factor,\" Accuracy\",acc,\" Macro F1:\",fs)\n",
        "      \n",
        "    #testdata['meddra_code'] = meddracode \n",
        "      #testdata['meddra_term'] = meddraterm \n",
        "    #print(testdata)\n",
        "    #testdata.to_csv(r\"FinalOutput.csv\")\n",
        "      #print(testdata)\n",
        "\n",
        "print(\"maxscore:\",maxscore)\n",
        "print(\"Medical Factor:\",alphaval,\" Tweet Factor:\",betaval)\n",
        "task3mix['cosine_meddra_code'] = meddracode"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Get the medical embeddings\n",
            "SANITY CHECK False\n",
            " Medical Sent: 0.9  Tweet Factor 1.0  Accuracy 0.4547945205479452  Macro F1: 0.3893429117361379\n",
            "maxscore: 0.3893429117361379\n",
            "Medical Factor: 0.9  Tweet Factor: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNy_GZn1hq9j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "task3mix['text'] = task3mix['text'].apply(lambda x: preprocess(x))\n",
        "task3mix['tweet'] = task3mix['tweet'].apply(lambda x: preprocess(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7dDQ9gAvDtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#task3mix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mhxLHxe8wxK",
        "colab_type": "text"
      },
      "source": [
        "CHECK THE Fscore on Training Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ns9FmN98cCn",
        "colab_type": "code",
        "outputId": "e4a39c86-f4b9-4b10-caec-a250a00423c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "a = [0.5]\n",
        "for i in range(len(a)):\n",
        "  factor = float(a[i])\n",
        "  #print(\"Get Train Set\")\n",
        "  testdata1 = pd.read_csv(r'./drive/My Drive/DataFolder/task3_training.tsv', sep='\\t')\n",
        "  #testdata1 = pd.read_csv(r'task3validation.csv')\n",
        "  testdata = testdata1.dropna(axis='columns', how='all')\n",
        "  meddracode = []\n",
        "  meddraterm = []\n",
        "  originalmeddracode = []\n",
        "  for index, row in testdata.iterrows():\n",
        "      getextract = row['extraction']\n",
        "      tweet = row['tweet']\n",
        "      if (isNotNan(getextract) and getextract!=''):\n",
        "        x = getembeddingssentencex(getextract,tweet,factor)\n",
        "        b = cosine_similarity(x,medsent)\n",
        "        maxval = np.argmax(b)\n",
        "        mc = int(medcode[int(maxval)])\n",
        "        mt = medtest[int(maxval)]\n",
        "        meddracode.append(int(mc))\n",
        "        meddraterm.append(mt)\n",
        "        originalmeddracode.append(int(row['meddra_code']))\n",
        "    \n",
        "  #print(\"Get accuracy score\")\n",
        "  #print(\"len of original\",len(originalmeddracode))\n",
        "  #print(\"len of predicted\",len(meddracode))\n",
        "  #print(\"Factor of tweet:\",factor)\n",
        "  print(\"Accuracy\",accuracy_score(originalmeddracode, meddracode))\n",
        "  print(\"Macro F1\",f1_score(originalmeddracode, meddracode, average='macro'))\n",
        "    \n",
        "  #testdata['meddra_code'] = meddracode \n",
        "    #testdata['meddra_term'] = meddraterm \n",
        "  #print(testdata)\n",
        "  #testdata.to_csv(r\"FinalOutput.csv\")\n",
        "    #print(testdata)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy 0.578551912568306\n",
            "Macro F1 0.5385954889619039\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9pVhnkU9Awm",
        "colab_type": "text"
      },
      "source": [
        "GIVE RESULTS FOR THE ACTUAL DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZOmoSsn9Egj",
        "colab_type": "code",
        "outputId": "d532a6f8-2e4b-41c1-c36f-9716004a897e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        }
      },
      "source": [
        "print(\"Get Set For Competition\")\n",
        "testdata1 = pd.read_csv(r'task3_op_before_meddra_mapping.tsv', sep='\\t')\n",
        "testdata2 = testdata1.dropna(axis='columns', how='all')\n",
        "testdata2['cleaned_tweet'] = testdata2['tweet'].apply(lambda x: preprocess(x))\n",
        "comp = testdata2\n",
        "comp\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Get Set For Competition\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>extract</th>\n",
              "      <th>type</th>\n",
              "      <th>tweet</th>\n",
              "      <th>cleaned_tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>332317478170546176</td>\n",
              "      <td>27.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>allergies</td>\n",
              "      <td>ADR</td>\n",
              "      <td>do you have any medication allergies? \"asthma!...</td>\n",
              "      <td>do you have any medication allergies ? asthma ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>347806215776116737</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>hurt your liver</td>\n",
              "      <td>ADR</td>\n",
              "      <td>@ashleylvivian if #avelox has hurt your liver,...</td>\n",
              "      <td>if a velox has hurt your liver , avoid tylenol...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>350336129817509888</td>\n",
              "      <td>75.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>length of focus today: about 30 seconds</td>\n",
              "      <td>ADR</td>\n",
              "      <td>apparently, baclofen greatly exacerbates the \"...</td>\n",
              "      <td>apparently , baclofen greatly exacerbates the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>348644040444637184</td>\n",
              "      <td>26.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>dreams</td>\n",
              "      <td>ADR</td>\n",
              "      <td>@bilgeebiri oh robitussin dreams are notorious...</td>\n",
              "      <td>oh robitussin dreams are notorious although wh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>352217412046823425</td>\n",
              "      <td>84.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>tendon rupture</td>\n",
              "      <td>ADR</td>\n",
              "      <td>#eds friends! anybody taken #cipro? (antibioti...</td>\n",
              "      <td>eds friends ! anybody taken cipro ? antibiotic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>434</th>\n",
              "      <td>331289838856835072</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>okay trazodone, work your magic.</td>\n",
              "      <td>okay trazodone , work your magic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>435</th>\n",
              "      <td>332907977700937731</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>tomorrow, sending mummers her mother's day pos...</td>\n",
              "      <td>tomorrow , sending mummers her mother s day po...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>436</th>\n",
              "      <td>333123450254274560</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@kyleecakes4 i was given like 5mg of abilify a...</td>\n",
              "      <td>i was given like 5 mg of abilify and i still d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>437</th>\n",
              "      <td>341950988455927808</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>stand down, tweeps! stand down! i did it wrong...</td>\n",
              "      <td>stand down , tweeps ! stand down ! i did it wr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>438</th>\n",
              "      <td>350709708484644864</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@loveone_another: i'm lowkey pill junkie. &amp;gt;...</td>\n",
              "      <td>i am lowkey pill junkie annoyed vivance , xana...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>439 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               tweet_id  ...                                      cleaned_tweet\n",
              "0    332317478170546176  ...  do you have any medication allergies ? asthma ...\n",
              "1    347806215776116737  ...  if a velox has hurt your liver , avoid tylenol...\n",
              "2    350336129817509888  ...  apparently , baclofen greatly exacerbates the ...\n",
              "3    348644040444637184  ...  oh robitussin dreams are notorious although wh...\n",
              "4    352217412046823425  ...  eds friends ! anybody taken cipro ? antibiotic...\n",
              "..                  ...  ...                                                ...\n",
              "434  331289838856835072  ...                   okay trazodone , work your magic\n",
              "435  332907977700937731  ...  tomorrow , sending mummers her mother s day po...\n",
              "436  333123450254274560  ...  i was given like 5 mg of abilify and i still d...\n",
              "437  341950988455927808  ...  stand down , tweeps ! stand down ! i did it wr...\n",
              "438  350709708484644864  ...  i am lowkey pill junkie annoyed vivance , xana...\n",
              "\n",
              "[439 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Byj67dLpoRlU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "meddracode = []\n",
        "meddraterm = []\n",
        "for i in range(len(comp)):\n",
        "    getextract = comp.iloc[i,3]\n",
        "    tweet = comp.iloc[i,5]\n",
        "    if (getextract!='' and isNotNan(getextract)):\n",
        "      #mc, mt = getTermAndCode(getextract) \n",
        "      x = getembeddingssentencex(getextract,tweet,0.8)\n",
        "      b = cosine_similarity(getembeddingssentence(getextract),medsent)\n",
        "      maxval = np.argmax(b)\n",
        "      mc = int(medcode[int(maxval)])\n",
        "      mt = medtest[int(maxval)]\n",
        "    else:\n",
        "      mc = \"\"\n",
        "      mt = \"\"\n",
        "    meddracode.append(mc)\n",
        "    meddraterm.append(mt)\n",
        "\n",
        "#print(len(meddracode))\n",
        "#print(len(meddraterm))\n",
        "\n",
        "comp['meddra_code_cosine'] = meddracode \n",
        "#testdata['meddra_term'] = meddraterm \n",
        "#print(testdata)\n",
        "#testdata.to_csv('./CosineFinal.tsv', sep='\\t', index=False)\n",
        "#print(testdata)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Agi8vSn4bhhW",
        "colab_type": "text"
      },
      "source": [
        "GET LOGISTIC REGRESSION & SVC EMBEDDINGS:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xgjk6uhblDW",
        "colab_type": "code",
        "outputId": "78557a39-9d1e-4688-877d-fc0c4a50fd3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "features = []\n",
        "for i in range(300):\n",
        "  features.append(\"column_\"+str(i))\n",
        "\n",
        "x = pd.read_csv('./drive/My Drive/Embeddings/extract/train/FastTextTrainingX.csv')[features]\n",
        "y = pd.read_csv('./drive/My Drive/Embeddings/extract/train/TrainingXLabel.csv')['label']\n",
        "\n",
        "xtrain = x\n",
        "ytrain = y\n",
        "\n",
        "print(xtrain.shape)\n",
        "print(ytrain.shape)\n",
        "\n",
        "x = pd.read_csv('./drive/My Drive/Embeddings/extract/test/FastTextTestingX.csv')[features]\n",
        "y = pd.read_csv('./drive/My Drive/Embeddings/extract/test/TestingXLabel.csv')['label']\n",
        "\n",
        "\n",
        "xtest = x\n",
        "ytest = y\n",
        "\n",
        "print(xtest.shape)\n",
        "print(ytest.shape)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "\n",
        "clf_logistic = LogisticRegression(random_state=0)\n",
        "clf_svc = LinearSVC(random_state=0)\n",
        "\n",
        "def logistic_regression_report(X_train_new, y_train_new, X_test, y_test):\n",
        "  clf_logistic.fit(X_train_new, y_train_new)\n",
        "\n",
        "  lr_pred = clf_logistic.predict(X_test)\n",
        "  original = []\n",
        "  for i in range(len(y_test)):\n",
        "   original.append(int(y_test[i]))\n",
        "  \n",
        "  print(\"MacroF1:\",f1_score(original, lr_pred, average='macro'))\n",
        "  print(\"MicroF1:\",f1_score(original, lr_pred, average='micro'))\n",
        "  print(\"MacroF1:\",f1_score(original, lr_pred, average='weighted'))\n",
        "  print((\"Accuracy:\",accuracy_score(original, lr_pred)))\n",
        "  return lr_pred\n",
        "\n",
        "\n",
        "\n",
        "def linear_svc_report(X_train_new, y_train_new, X_test, y_test):\n",
        "  \n",
        "  clf_svc.fit(X_train_new, y_train_new)\n",
        "\n",
        "  svm_pred = clf_svc.predict(X_test)\n",
        "  original = []\n",
        "  for i in range(len(y_test)):\n",
        "   original.append(int(y_test[i]))\n",
        "\n",
        "  print(\"MacroF1:\",f1_score(original, svm_pred, average='macro'))\n",
        "  print(\"MicroF1:\",f1_score(original, svm_pred, average='micro'))\n",
        "  print(\"weightedF1:\",f1_score(original, svm_pred, average='weighted'))\n",
        "  print((\"Accuracy:\",accuracy_score(original, svm_pred)))\n",
        "  return svm_pred\n",
        "  #print(classification_report(y_test, svm_pred))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10847, 300)\n",
            "(10847,)\n",
            "(365, 300)\n",
            "(365,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHFOfct9c0yv",
        "colab_type": "code",
        "outputId": "d93112a4-ef58-41eb-ab7f-490ec47afd58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "task3mix['logistic_meddra_code'] = logistic_regression_report(xtrain, ytrain, xtest, ytest)\n",
        "task3mix['svc_meddra_code'] = linear_svc_report(xtrain, ytrain, xtest, ytest)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MacroF1: 0.3463952016127893\n",
            "MicroF1: 0.46301369863013697\n",
            "MacroF1: 0.44623127148398756\n",
            "('Accuracy:', 0.46301369863013697)\n",
            "MacroF1: 0.32305062260758466\n",
            "MicroF1: 0.4575342465753425\n",
            "weightedF1: 0.44133564152742233\n",
            "('Accuracy:', 0.4575342465753425)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeCoXwPb4OpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "task3mix['logistic_meddra_code'] = task3mix['logistic_meddra_code'].apply(lambda x: numbertolabel(x))\n",
        "task3mix['svc_meddra_code'] = task3mix['svc_meddra_code'].apply(lambda x: numbertolabel(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Dfq5cquiOjM",
        "colab_type": "text"
      },
      "source": [
        "Try To get the Logistic and SVC for test data - COMP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoJZKCRqdQn8",
        "colab_type": "code",
        "outputId": "1c3375e5-6b89-4d83-cf7e-545ee6524c37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "medicalfinal = comp\n",
        "FastTextfeatures = []\n",
        "for i in range(300):\n",
        "  FastTextfeatures.append(\"column_\"+str(i))\n",
        "\n",
        "fastTextTrainingX = pd.DataFrame(columns=FastTextfeatures)\n",
        "#TrainingXLabel = pd.DataFrame(columns=['label'])\n",
        "\n",
        "for i in range(len(medicalfinal)):\n",
        "  #print(\"I\",i)\n",
        "  extract = (medicalfinal.iloc[i,3])\n",
        "  #print(extract)\n",
        "  #tweet = (medicalfinal.iloc[i,5])\n",
        "  #label = (medicalfinal.iloc[i,1])\n",
        "  if isNotNan(extract):\n",
        "    #print(extract)\n",
        "    extract = preprocess(extract)\n",
        "    #tweet = clean(tweet)\n",
        "    #label = int(label)\n",
        "    \n",
        "    a = np.reshape((embedder((extract))), (1, 300))\n",
        "    df = pd.DataFrame(a,columns=FastTextfeatures)\n",
        "    fastTextTrainingX = fastTextTrainingX.append(df,ignore_index = True)\n",
        "\n",
        "print(fastTextTrainingX.shape)\n",
        "X_test = fastTextTrainingX\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(201, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kq8QjWZQZUuS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logistic_predicted = clf_logistic.predict(X_test)\n",
        "svc_predicted = clf_svc.predict(X_test)\n",
        "\n",
        "add_predicted_logistic = []\n",
        "add_predicted_svc = []\n",
        "t = 0\n",
        "\n",
        "for i in range(len(comp)):\n",
        "    getextract = comp.iloc[i,3]\n",
        "    if isNotNan(getextract):\n",
        "      add_predicted_logistic.append(numbertolabel(logistic_predicted[t]))\n",
        "      add_predicted_svc.append(numbertolabel(svc_predicted[t]))\n",
        "      t = t + 1\n",
        "    else:\n",
        "      add_predicted_logistic.append(\"\")\n",
        "      add_predicted_svc.append(\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgo7SaA3ZP9T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "comp['logistic_meddra_code'] = add_predicted_logistic\n",
        "comp['svc_meddra_code'] = add_predicted_svc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz1FONaUdyfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#comp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FpoI9ANnC6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#task3mix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wy_YKV0_Wa_",
        "colab_type": "text"
      },
      "source": [
        "CHARACTER RNN + FASTTEXT EMBEDDINGS: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljNkwoRW_bfL",
        "colab_type": "code",
        "outputId": "ace6c132-9832-4f22-d09f-4da0a4af8415",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "task1_tweets = pd.read_csv('./drive/My Drive/DataFolder/task1_training.tsv', sep='\\t')[['tweet']]\n",
        "print(\"task1_tweets:\",task1_tweets.shape)\n",
        "#print(\"task1_tweets:\",task1_tweets.head(2))\n",
        "task2_tweets = pd.read_csv('./drive/My Drive/DataFolder/task2_en_training.tsv', sep='\\t')[['tweet']]\n",
        "print(\"task2_tweets:\",task2_tweets.shape)\n",
        "#print(\"task2_tweets:\",task2_tweets.head(2))\n",
        "\n",
        "x2 = pd.read_csv('./drive/My Drive/DataFolder/task3_training.tsv', sep='\\t')[['extraction','meddra_code','tweet']]\n",
        "x2.dropna(inplace = True)\n",
        "x2.columns = ['text', 'label','tweet']\n",
        "x2['text'] = x2['text'].apply(preprocess)\n",
        "x2['tweet'] = x2['tweet'].apply(preprocess)\n",
        "x2['label'] = x2['label'].apply(cleannumber)\n",
        "print(\"x2 shape:\",x2.shape)\n",
        "#print(\"x2:\",x2.head(2))\n",
        "\n",
        "expanded_data = pd.read_csv('./drive/My Drive/DataFolder/TrainingExampleSampledV3.csv')[['text', 'label', 'tweet']].drop_duplicates()\n",
        "expanded_data_tweets = pd.read_csv('./drive/My Drive/DataFolder/TrainingExampleSampledV3.csv')[['tweet']].drop_duplicates()\n",
        "#expanded_data_tweets = pd.read_csv('./drive/My Drive/DataFolder/Cadec.csv')[['text']].drop_duplicates()\n",
        "expanded_data.dropna(inplace=True)\n",
        "expanded_data['text'] = expanded_data['text'].apply(preprocess)\n",
        "expanded_data['tweet'] = expanded_data['tweet'].apply(preprocess)\n",
        "expanded_data['label'] = expanded_data['label'].apply(cleannumber)\n",
        "print(\"expanded_data shape:\",expanded_data.shape)\n",
        "expand_merged = expanded_data[expanded_data['tweet'] != 'name ?'].merge(x2, how ='left', on='tweet')\n",
        "expand_merged = expand_merged[expand_merged['text_y'].isna()]\n",
        "data_to_add = expand_merged[['text_x', 'label_x', 'tweet']].drop_duplicates()\n",
        "data_to_add.columns = ['text', 'label', 'tweet']\n",
        "print(\"data to add shape:\",data_to_add.shape)\n",
        "\n",
        "x2 = pd.concat([x2, data_to_add])\n",
        "print(\"x2 shape:\",x2.shape)\n",
        "#print(\"x2:\",x2.head(2))\n",
        "\n",
        "all_tweets = pd.concat([task1_tweets,expanded_data_tweets, task2_tweets,x2[['tweet']]]).drop_duplicates()\n",
        "all_tweets.dropna(inplace=True)\n",
        "all_tweets['tweet'] = all_tweets['tweet'].apply(lambda x: preprocess(x))\n",
        "print(\"all_tweets_shape:\",all_tweets.shape)\n",
        "#print(\"all_tweets\",all_tweets.head(2))\n",
        "\n",
        "validation = pd.read_csv('./drive/My Drive/DataFolder/task3_validation.tsv', sep='\\t')[['extraction', 'meddra_code','tweet']].drop_duplicates()\n",
        "validation.columns = ['text', 'label','tweet']\n",
        "validation. dropna(subset = [\"text\"], inplace=True)\n",
        "#validation['text'] = validation['text'].apply(data)\n",
        "validation['text'] = validation['text'].apply(preprocess)\n",
        "validation['tweet'] = validation['tweet'].apply(preprocess)\n",
        "validation['label'] = validation['label'].apply(cleannumber)\n",
        "print(\"validation shape:\",validation.shape)\n",
        "#print(\"validation\",validation.head(2))\n",
        "validation.dropna(inplace=True)\n",
        "print(\"Validation shape:\",validation.shape)\n",
        "\n",
        "validation = validation.astype({\"text\": object, \"label\": int})\n",
        "#print(\"validation\",validation)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder = LabelEncoder()\n",
        "\n",
        "all_labels = pd.DataFrame(pd.concat([validation['label'], x2['label']]),columns = ['label']).drop_duplicates()\n",
        "all_labels['label_encoded'] = labelencoder.fit_transform(all_labels['label'])\n",
        "print(\"All_lables_shape\",all_labels.shape)\n",
        "print(\"ALL Labels:\",all_labels.head(2))\n",
        "\n",
        "validation = validation.merge(all_labels, how = 'inner', on = 'label')\n",
        "x2 = x2.merge(all_labels, how = 'inner', on = 'label')\n",
        "print(\"X2:\",x2.head(2))\n",
        "print(\"Validation\",validation.head(2))\n",
        "\n",
        "print(\"X2:\",x2.shape)\n",
        "print(\"labels = \",len(set(list(x2.label))))\n",
        "\n",
        "train_label_distribution = pd.DataFrame(x2.label.value_counts()).reset_index()\n",
        "train_label_distribution.columns = ['label', 'freq']\n",
        "print(\"train_label_distribution shape:\",train_label_distribution.shape)\n",
        "print(\"train_label_distribution:\",train_label_distribution.head(2))\n",
        "\n",
        "valid_label_distribution = pd.DataFrame(validation.label.value_counts()).reset_index()\n",
        "valid_label_distribution.columns = ['label', 'freq']\n",
        "print(\"valid_label_distribution shape:\",valid_label_distribution.shape)\n",
        "print(\"valid_label_distribution\",valid_label_distribution.head(2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "task1_tweets: (55419, 1)\n",
            "task2_tweets: (20544, 1)\n",
            "x2 shape: (1464, 3)\n",
            "expanded_data shape: (12453, 3)\n",
            "data to add shape: (11030, 3)\n",
            "x2 shape: (12494, 3)\n",
            "all_tweets_shape: (77759, 1)\n",
            "validation shape: (365, 3)\n",
            "Validation shape: (365, 3)\n",
            "All_lables_shape (475, 2)\n",
            "ALL Labels:       label  label_encoded\n",
            "0  10013661             98\n",
            "1  10024668            215\n",
            "X2:                        text  ...  label_encoded\n",
            "0               lost vision  ...            388\n",
            "1  loss vision things occur  ...            388\n",
            "\n",
            "[2 rows x 4 columns]\n",
            "Validation         text  ...  label_encoded\n",
            "0  allergies  ...             98\n",
            "1   allergic  ...             98\n",
            "\n",
            "[2 rows x 4 columns]\n",
            "X2: (12494, 4)\n",
            "labels =  475\n",
            "train_label_distribution shape: (475, 2)\n",
            "train_label_distribution:       label  freq\n",
            "0  10033371   117\n",
            "1  10016256    97\n",
            "valid_label_distribution shape: (183, 2)\n",
            "valid_label_distribution       label  freq\n",
            "0  10041349    16\n",
            "1  10048010    14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REJfWeIXHsHR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sns.distplot(train_label_distribution['freq'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mPBPHMdHsvI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sns.distplot(valid_label_distribution['freq'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzpNH80THuOI",
        "colab_type": "code",
        "outputId": "36c16a25-dfaa-451f-ebb1-94b75775fc6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"print(np.percentile(train_label_distribution['freq'],0))\n",
        "print(np.percentile(train_label_distribution['freq'],10))\n",
        "print(np.percentile(train_label_distribution['freq'],50))\n",
        "print(np.percentile(train_label_distribution['freq'],90))\n",
        "print(np.percentile(train_label_distribution['freq'],95))\n",
        "print(np.percentile(train_label_distribution['freq'],99))\n",
        "print(np.percentile(train_label_distribution['freq'],99.99))\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"print(np.percentile(train_label_distribution['freq'],0))\\nprint(np.percentile(train_label_distribution['freq'],10))\\nprint(np.percentile(train_label_distribution['freq'],50))\\nprint(np.percentile(train_label_distribution['freq'],90))\\nprint(np.percentile(train_label_distribution['freq'],95))\\nprint(np.percentile(train_label_distribution['freq'],99))\\nprint(np.percentile(train_label_distribution['freq'],99.99))\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWfRgbkpHwaL",
        "colab_type": "code",
        "outputId": "d1710e15-6ef1-4a19-ddde-bc942a08dc27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"print(np.percentile(valid_label_distribution['freq'],0))\n",
        "print(np.percentile(valid_label_distribution['freq'],10))\n",
        "print(np.percentile(valid_label_distribution['freq'],50))\n",
        "print(np.percentile(valid_label_distribution['freq'],90))\n",
        "print(np.percentile(valid_label_distribution['freq'],95))\n",
        "print(np.percentile(valid_label_distribution['freq'],99))\n",
        "print(np.percentile(valid_label_distribution['freq'],99.99))\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"print(np.percentile(valid_label_distribution['freq'],0))\\nprint(np.percentile(valid_label_distribution['freq'],10))\\nprint(np.percentile(valid_label_distribution['freq'],50))\\nprint(np.percentile(valid_label_distribution['freq'],90))\\nprint(np.percentile(valid_label_distribution['freq'],95))\\nprint(np.percentile(valid_label_distribution['freq'],99))\\nprint(np.percentile(valid_label_distribution['freq'],99.99))\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-EeCO3EHxyn",
        "colab_type": "code",
        "outputId": "edee562e-e72f-48ac-bf74-879b729ac30b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "matched_labels = valid_label_distribution.merge(train_label_distribution, how ='inner',\n",
        "                               on='label')\n",
        "print(train_label_distribution.shape, valid_label_distribution.shape, matched_labels.shape)\n",
        "matched_labels.head(2)\n",
        "\n",
        "un_matched_labels = train_label_distribution.merge(valid_label_distribution, how ='left',\n",
        "                               on='label')\n",
        "un_matched_labels = un_matched_labels[un_matched_labels['freq_y'].isna()]\n",
        "print(un_matched_labels.shape)\n",
        "un_matched_labels.head(2)\n",
        "\n",
        "matched_labels.corr()\n",
        "matched_labels.to_csv('smm4h_task3_correlations.csv', index = False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(475, 2) (183, 2) (183, 3)\n",
            "(292, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2SW4DUTH4ko",
        "colab_type": "code",
        "outputId": "8b78116e-7e30-410c-ba71-26e24fe06fbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"print(np.percentile(matched_labels['freq_x'],0))\n",
        "print(np.percentile(matched_labels['freq_x'],10))\n",
        "print(np.percentile(matched_labels['freq_x'],50))\n",
        "print(np.percentile(matched_labels['freq_x'],90))\n",
        "print(np.percentile(matched_labels['freq_x'],95))\n",
        "print(np.percentile(matched_labels['freq_x'],99))\n",
        "print(np.percentile(matched_labels['freq_x'],99.99))\"\"\"\n",
        "\n",
        "\"\"\"print(np.percentile(matched_labels['freq_y'],0))\n",
        "print(np.percentile(matched_labels['freq_y'],10))\n",
        "print(np.percentile(matched_labels['freq_y'],50))\n",
        "print(np.percentile(matched_labels['freq_y'],90))\n",
        "print(np.percentile(matched_labels['freq_y'],95))\n",
        "print(np.percentile(matched_labels['freq_y'],99))\n",
        "print(np.percentile(matched_labels['freq_y'],99.99))\"\"\"\n",
        "\n",
        "\"\"\"print(np.percentile(un_matched_labels['freq_x'],0))\n",
        "print(np.percentile(un_matched_labels['freq_x'],10))\n",
        "print(np.percentile(un_matched_labels['freq_x'],50))\n",
        "print(np.percentile(un_matched_labels['freq_x'],90))\n",
        "print(np.percentile(un_matched_labels['freq_x'],95))\n",
        "print(np.percentile(un_matched_labels['freq_x'],99))\n",
        "print(np.percentile(un_matched_labels['freq_x'],99.99))\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"print(np.percentile(un_matched_labels['freq_x'],0))\\nprint(np.percentile(un_matched_labels['freq_x'],10))\\nprint(np.percentile(un_matched_labels['freq_x'],50))\\nprint(np.percentile(un_matched_labels['freq_x'],90))\\nprint(np.percentile(un_matched_labels['freq_x'],95))\\nprint(np.percentile(un_matched_labels['freq_x'],99))\\nprint(np.percentile(un_matched_labels['freq_x'],99.99))\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYp7Qm5444Vh",
        "colab_type": "code",
        "outputId": "e7fff339-77cf-42fa-8193-5d4eb19a3708",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "all_tweets.head(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>these new anti anxiety meds make me so sleepy ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>you can try vitamins for olivia ! hudson has t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweet\n",
              "0  these new anti anxiety meds make me so sleepy ...\n",
              "1  you can try vitamins for olivia ! hudson has t..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zO6vj8XzIDOk",
        "colab_type": "code",
        "outputId": "f9230d6a-e2c9-493a-89e5-d764bf843d72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tweet_corpus = []\n",
        "for ix, row in all_tweets.iterrows():\n",
        "  tweet_corpus.append(row['tweet'].split())\n",
        "print(len(tweet_corpus))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "77759\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oG-mygw233Gy",
        "colab_type": "code",
        "outputId": "76be6574-da1d-49b2-c16f-d3924929d61a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import gensim, json, nltk\n",
        "from gensim.models import FastText\n",
        "from gensim.test.utils import get_tmpfile\n",
        "\n",
        "all_word_model_ft = FastText(tweet_corpus, size=200, window=5, min_count=1, workers=8,sg=1, iter=50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "129 | INFO | adding document #0 to Dictionary(0 unique tokens: [])\n",
            "129 | INFO | built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)\n",
            "129 | INFO | collecting all words and their counts\n",
            "129 | INFO | PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "129 | INFO | PROGRESS: at sentence #10000, processed 122486 words, keeping 9899 word types\n",
            "129 | INFO | PROGRESS: at sentence #20000, processed 245642 words, keeping 14171 word types\n",
            "129 | INFO | PROGRESS: at sentence #30000, processed 369539 words, keeping 17305 word types\n",
            "129 | INFO | PROGRESS: at sentence #40000, processed 492625 words, keeping 19653 word types\n",
            "129 | INFO | PROGRESS: at sentence #50000, processed 616111 words, keeping 21872 word types\n",
            "129 | INFO | PROGRESS: at sentence #60000, processed 834719 words, keeping 27524 word types\n",
            "129 | INFO | PROGRESS: at sentence #70000, processed 1004539 words, keeping 33934 word types\n",
            "129 | INFO | collected 37350 word types from a corpus of 1206392 raw words and 77759 sentences\n",
            "129 | INFO | Loading a fresh vocabulary\n",
            "129 | INFO | effective_min_count=1 retains 37350 unique words (100% of original 37350, drops 0)\n",
            "129 | INFO | effective_min_count=1 leaves 1206392 word corpus (100% of original 1206392, drops 0)\n",
            "129 | INFO | deleting the raw counts dictionary of 37350 items\n",
            "129 | INFO | sample=0.001 downsamples 49 most-common words\n",
            "129 | INFO | downsampling leaves estimated 929136 word corpus (77.0% of prior 1206392)\n",
            "129 | INFO | estimated required memory for 37350 words, 206859 buckets and 200 dimensions: 252117168 bytes\n",
            "129 | INFO | resetting layer weights\n",
            "129 | INFO | Total number of ngrams is 206859\n",
            "129 | INFO | training model with 8 workers on 37350 vocabulary and 200 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
            "129 | INFO | EPOCH 1 - PROGRESS: at 9.42% examples, 68479 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 1 - PROGRESS: at 21.99% examples, 78496 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 1 - PROGRESS: at 36.54% examples, 84917 words/s, in_qsize 15, out_qsize 1\n",
            "129 | INFO | EPOCH 1 - PROGRESS: at 52.21% examples, 90569 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 1 - PROGRESS: at 65.71% examples, 90911 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 1 - PROGRESS: at 72.09% examples, 87997 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 1 - PROGRESS: at 79.75% examples, 86712 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 1 - PROGRESS: at 88.76% examples, 86613 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 1 - PROGRESS: at 97.86% examples, 85245 words/s, in_qsize 9, out_qsize 1\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 1 : training on 1206392 raw words (928692 effective words) took 10.6s, 87910 effective words/s\n",
            "129 | INFO | EPOCH 2 - PROGRESS: at 10.47% examples, 57537 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 2 - PROGRESS: at 27.16% examples, 78271 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 2 - PROGRESS: at 43.82% examples, 84427 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 2 - PROGRESS: at 60.48% examples, 86186 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 2 - PROGRESS: at 71.83% examples, 90683 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 2 - PROGRESS: at 75.94% examples, 87085 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 2 - PROGRESS: at 84.12% examples, 86512 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 2 - PROGRESS: at 94.17% examples, 87142 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | EPOCH 2 - PROGRESS: at 99.51% examples, 86516 words/s, in_qsize 4, out_qsize 1\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 2 : training on 1206392 raw words (929204 effective words) took 10.5s, 88416 effective words/s\n",
            "129 | INFO | EPOCH 3 - PROGRESS: at 9.46% examples, 58512 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 3 - PROGRESS: at 26.16% examples, 80419 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 3 - PROGRESS: at 42.79% examples, 88567 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 3 - PROGRESS: at 56.41% examples, 90879 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 3 - PROGRESS: at 69.82% examples, 92125 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 3 - PROGRESS: at 73.10% examples, 88724 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 3 - PROGRESS: at 81.94% examples, 87599 words/s, in_qsize 13, out_qsize 2\n",
            "129 | INFO | EPOCH 3 - PROGRESS: at 91.83% examples, 88461 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 3 - PROGRESS: at 99.16% examples, 86846 words/s, in_qsize 6, out_qsize 3\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 3 : training on 1206392 raw words (929282 effective words) took 10.3s, 90408 effective words/s\n",
            "129 | INFO | EPOCH 4 - PROGRESS: at 9.42% examples, 61030 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 4 - PROGRESS: at 26.16% examples, 80859 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 4 - PROGRESS: at 42.79% examples, 85621 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 4 - PROGRESS: at 59.50% examples, 89121 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 4 - PROGRESS: at 71.50% examples, 90264 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 4 - PROGRESS: at 75.20% examples, 89103 words/s, in_qsize 15, out_qsize 1\n",
            "129 | INFO | EPOCH 4 - PROGRESS: at 84.12% examples, 86493 words/s, in_qsize 14, out_qsize 1\n",
            "129 | INFO | EPOCH 4 - PROGRESS: at 94.92% examples, 88446 words/s, in_qsize 14, out_qsize 0\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | EPOCH 4 - PROGRESS: at 99.76% examples, 88426 words/s, in_qsize 2, out_qsize 1\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 4 : training on 1206392 raw words (928668 effective words) took 10.4s, 89696 effective words/s\n",
            "129 | INFO | EPOCH 5 - PROGRESS: at 9.42% examples, 60698 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 5 - PROGRESS: at 26.16% examples, 83380 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 5 - PROGRESS: at 42.79% examples, 87741 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 5 - PROGRESS: at 59.50% examples, 91913 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 5 - PROGRESS: at 71.29% examples, 90834 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 5 - PROGRESS: at 75.20% examples, 89543 words/s, in_qsize 16, out_qsize 1\n",
            "129 | INFO | EPOCH 5 - PROGRESS: at 82.67% examples, 88113 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 5 - PROGRESS: at 91.83% examples, 88505 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 5 - PROGRESS: at 99.16% examples, 88129 words/s, in_qsize 7, out_qsize 0\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 5 : training on 1206392 raw words (928857 effective words) took 10.3s, 89884 effective words/s\n",
            "129 | INFO | EPOCH 6 - PROGRESS: at 9.42% examples, 68031 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 6 - PROGRESS: at 22.02% examples, 78874 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 6 - PROGRESS: at 35.48% examples, 84040 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 6 - PROGRESS: at 52.21% examples, 88398 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 6 - PROGRESS: at 68.79% examples, 90565 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 6 - PROGRESS: at 72.33% examples, 87760 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 6 - PROGRESS: at 81.94% examples, 88482 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 6 - PROGRESS: at 90.31% examples, 87182 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 6 - PROGRESS: at 99.16% examples, 87810 words/s, in_qsize 7, out_qsize 0\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 6 : training on 1206392 raw words (928677 effective words) took 10.4s, 89599 effective words/s\n",
            "129 | INFO | EPOCH 7 - PROGRESS: at 9.42% examples, 67214 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 7 - PROGRESS: at 23.02% examples, 80345 words/s, in_qsize 13, out_qsize 2\n",
            "129 | INFO | EPOCH 7 - PROGRESS: at 36.54% examples, 85068 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 7 - PROGRESS: at 53.25% examples, 87914 words/s, in_qsize 14, out_qsize 1\n",
            "129 | INFO | EPOCH 7 - PROGRESS: at 69.82% examples, 90329 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 7 - PROGRESS: at 73.66% examples, 88168 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 7 - PROGRESS: at 82.67% examples, 87822 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 7 - PROGRESS: at 91.83% examples, 86922 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | EPOCH 7 - PROGRESS: at 99.27% examples, 86832 words/s, in_qsize 6, out_qsize 1\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 7 : training on 1206392 raw words (928960 effective words) took 10.4s, 89572 effective words/s\n",
            "129 | INFO | EPOCH 8 - PROGRESS: at 10.47% examples, 61464 words/s, in_qsize 15, out_qsize 1\n",
            "129 | INFO | EPOCH 8 - PROGRESS: at 27.20% examples, 78847 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 8 - PROGRESS: at 44.84% examples, 89588 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 8 - PROGRESS: at 59.50% examples, 93034 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 8 - PROGRESS: at 71.29% examples, 91632 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 8 - PROGRESS: at 74.45% examples, 90181 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 8 - PROGRESS: at 83.40% examples, 89550 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 8 - PROGRESS: at 90.31% examples, 86719 words/s, in_qsize 13, out_qsize 2\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | EPOCH 8 - PROGRESS: at 99.27% examples, 88438 words/s, in_qsize 6, out_qsize 1\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 8 : training on 1206392 raw words (929083 effective words) took 10.3s, 90153 effective words/s\n",
            "129 | INFO | EPOCH 9 - PROGRESS: at 10.47% examples, 73455 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 9 - PROGRESS: at 26.16% examples, 87562 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 9 - PROGRESS: at 37.56% examples, 83378 words/s, in_qsize 16, out_qsize 1\n",
            "129 | INFO | EPOCH 9 - PROGRESS: at 53.25% examples, 90159 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 9 - PROGRESS: at 67.75% examples, 92829 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 9 - PROGRESS: at 72.21% examples, 86882 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 9 - PROGRESS: at 81.94% examples, 89617 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 9 - PROGRESS: at 89.54% examples, 85974 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 9 - PROGRESS: at 99.16% examples, 88161 words/s, in_qsize 7, out_qsize 1\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 9 : training on 1206392 raw words (929318 effective words) took 10.5s, 88909 effective words/s\n",
            "129 | INFO | EPOCH 10 - PROGRESS: at 10.47% examples, 68456 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 10 - PROGRESS: at 26.16% examples, 89764 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 10 - PROGRESS: at 36.55% examples, 82691 words/s, in_qsize 14, out_qsize 1\n",
            "129 | INFO | EPOCH 10 - PROGRESS: at 53.25% examples, 86282 words/s, in_qsize 14, out_qsize 1\n",
            "129 | INFO | EPOCH 10 - PROGRESS: at 69.82% examples, 88709 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 10 - PROGRESS: at 75.94% examples, 92298 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 10 - PROGRESS: at 82.67% examples, 86514 words/s, in_qsize 15, out_qsize 1\n",
            "129 | INFO | EPOCH 10 - PROGRESS: at 92.59% examples, 88043 words/s, in_qsize 16, out_qsize 1\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | EPOCH 10 - PROGRESS: at 99.27% examples, 85838 words/s, in_qsize 6, out_qsize 1\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 10 : training on 1206392 raw words (929222 effective words) took 10.4s, 89284 effective words/s\n",
            "129 | INFO | EPOCH 11 - PROGRESS: at 10.47% examples, 68429 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 11 - PROGRESS: at 26.16% examples, 87368 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 11 - PROGRESS: at 37.56% examples, 86237 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 11 - PROGRESS: at 53.25% examples, 89410 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 11 - PROGRESS: at 68.79% examples, 93045 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 11 - PROGRESS: at 72.21% examples, 89668 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 11 - PROGRESS: at 79.75% examples, 88389 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 11 - PROGRESS: at 88.76% examples, 88428 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 11 - PROGRESS: at 96.45% examples, 86594 words/s, in_qsize 12, out_qsize 0\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 11 : training on 1206392 raw words (928711 effective words) took 10.4s, 89591 effective words/s\n",
            "129 | INFO | EPOCH 12 - PROGRESS: at 10.47% examples, 65839 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 12 - PROGRESS: at 26.16% examples, 87998 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 12 - PROGRESS: at 38.61% examples, 86582 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 12 - PROGRESS: at 52.21% examples, 89106 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 12 - PROGRESS: at 64.66% examples, 89339 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 12 - PROGRESS: at 72.09% examples, 89142 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 12 - PROGRESS: at 78.98% examples, 86321 words/s, in_qsize 15, out_qsize 3\n",
            "129 | INFO | EPOCH 12 - PROGRESS: at 88.76% examples, 88279 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 12 - PROGRESS: at 97.13% examples, 85448 words/s, in_qsize 11, out_qsize 0\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 12 : training on 1206392 raw words (929614 effective words) took 10.4s, 89213 effective words/s\n",
            "129 | INFO | EPOCH 13 - PROGRESS: at 9.43% examples, 60561 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 13 - PROGRESS: at 26.13% examples, 81537 words/s, in_qsize 14, out_qsize 1\n",
            "129 | INFO | EPOCH 13 - PROGRESS: at 42.79% examples, 86634 words/s, in_qsize 16, out_qsize 1\n",
            "129 | INFO | EPOCH 13 - PROGRESS: at 59.50% examples, 90402 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 13 - PROGRESS: at 71.50% examples, 89998 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 13 - PROGRESS: at 75.20% examples, 88984 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 13 - PROGRESS: at 84.12% examples, 88199 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 13 - PROGRESS: at 93.37% examples, 88035 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | EPOCH 13 - PROGRESS: at 99.51% examples, 87499 words/s, in_qsize 4, out_qsize 1\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 13 : training on 1206392 raw words (929257 effective words) took 10.4s, 89709 effective words/s\n",
            "129 | INFO | EPOCH 14 - PROGRESS: at 9.42% examples, 67364 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 14 - PROGRESS: at 20.96% examples, 74269 words/s, in_qsize 16, out_qsize 1\n",
            "129 | INFO | EPOCH 14 - PROGRESS: at 36.52% examples, 82339 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 14 - PROGRESS: at 52.21% examples, 88988 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 14 - PROGRESS: at 68.79% examples, 91603 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 14 - PROGRESS: at 72.09% examples, 86985 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 14 - PROGRESS: at 81.94% examples, 88719 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 14 - PROGRESS: at 89.54% examples, 87355 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 14 - PROGRESS: at 99.16% examples, 88183 words/s, in_qsize 7, out_qsize 1\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 14 : training on 1206392 raw words (928992 effective words) took 10.4s, 89583 effective words/s\n",
            "129 | INFO | EPOCH 15 - PROGRESS: at 10.47% examples, 65159 words/s, in_qsize 15, out_qsize 1\n",
            "129 | INFO | EPOCH 15 - PROGRESS: at 26.16% examples, 87776 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 15 - PROGRESS: at 38.61% examples, 88299 words/s, in_qsize 16, out_qsize 2\n",
            "129 | INFO | EPOCH 15 - PROGRESS: at 52.21% examples, 89694 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 15 - PROGRESS: at 68.79% examples, 92274 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 15 - PROGRESS: at 72.21% examples, 89419 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 15 - PROGRESS: at 81.94% examples, 89009 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 15 - PROGRESS: at 90.31% examples, 88131 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 15 - PROGRESS: at 99.16% examples, 88367 words/s, in_qsize 7, out_qsize 1\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 15 : training on 1206392 raw words (929206 effective words) took 10.3s, 90592 effective words/s\n",
            "129 | INFO | EPOCH 16 - PROGRESS: at 9.42% examples, 62040 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 16 - PROGRESS: at 26.16% examples, 79814 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 16 - PROGRESS: at 42.79% examples, 88660 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 16 - PROGRESS: at 57.46% examples, 92191 words/s, in_qsize 14, out_qsize 1\n",
            "129 | INFO | EPOCH 16 - PROGRESS: at 70.60% examples, 91981 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 16 - PROGRESS: at 73.66% examples, 90124 words/s, in_qsize 14, out_qsize 1\n",
            "129 | INFO | EPOCH 16 - PROGRESS: at 81.94% examples, 89039 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 16 - PROGRESS: at 90.31% examples, 88139 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 16 - PROGRESS: at 99.03% examples, 88753 words/s, in_qsize 8, out_qsize 0\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 16 : training on 1206392 raw words (929175 effective words) took 10.3s, 90101 effective words/s\n",
            "129 | INFO | EPOCH 17 - PROGRESS: at 10.47% examples, 64217 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 17 - PROGRESS: at 27.16% examples, 84430 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 17 - PROGRESS: at 38.61% examples, 83846 words/s, in_qsize 14, out_qsize 1\n",
            "129 | INFO | EPOCH 17 - PROGRESS: at 54.32% examples, 88594 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 17 - PROGRESS: at 69.82% examples, 91079 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 17 - PROGRESS: at 72.33% examples, 86615 words/s, in_qsize 16, out_qsize 1\n",
            "129 | INFO | EPOCH 17 - PROGRESS: at 81.93% examples, 88196 words/s, in_qsize 15, out_qsize 1\n",
            "129 | INFO | EPOCH 17 - PROGRESS: at 90.31% examples, 87057 words/s, in_qsize 15, out_qsize 1\n",
            "129 | INFO | EPOCH 17 - PROGRESS: at 99.03% examples, 86955 words/s, in_qsize 8, out_qsize 0\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 17 : training on 1206392 raw words (928997 effective words) took 10.4s, 89398 effective words/s\n",
            "129 | INFO | EPOCH 18 - PROGRESS: at 10.47% examples, 61482 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 18 - PROGRESS: at 27.16% examples, 87002 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 18 - PROGRESS: at 42.79% examples, 92767 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 18 - PROGRESS: at 53.28% examples, 88133 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 18 - PROGRESS: at 69.82% examples, 90455 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 18 - PROGRESS: at 75.20% examples, 92769 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 18 - PROGRESS: at 82.67% examples, 88152 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 18 - PROGRESS: at 91.07% examples, 87870 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | EPOCH 18 - PROGRESS: at 99.27% examples, 87854 words/s, in_qsize 6, out_qsize 0\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 18 : training on 1206392 raw words (928769 effective words) took 10.2s, 90702 effective words/s\n",
            "129 | INFO | EPOCH 19 - PROGRESS: at 9.42% examples, 64342 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 19 - PROGRESS: at 26.16% examples, 85745 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 19 - PROGRESS: at 42.79% examples, 91033 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 19 - PROGRESS: at 55.40% examples, 88600 words/s, in_qsize 11, out_qsize 4\n",
            "129 | INFO | EPOCH 19 - PROGRESS: at 70.60% examples, 92063 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 19 - PROGRESS: at 72.90% examples, 89321 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 19 - PROGRESS: at 83.40% examples, 89312 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 19 - PROGRESS: at 91.09% examples, 87470 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | EPOCH 19 - PROGRESS: at 99.38% examples, 88389 words/s, in_qsize 5, out_qsize 1\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 19 : training on 1206392 raw words (929338 effective words) took 10.3s, 90535 effective words/s\n",
            "129 | INFO | EPOCH 20 - PROGRESS: at 9.43% examples, 59482 words/s, in_qsize 16, out_qsize 1\n",
            "129 | INFO | EPOCH 20 - PROGRESS: at 26.16% examples, 85936 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 20 - PROGRESS: at 42.79% examples, 90982 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 20 - PROGRESS: at 54.32% examples, 89103 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 20 - PROGRESS: at 69.82% examples, 90629 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 20 - PROGRESS: at 72.78% examples, 87892 words/s, in_qsize 14, out_qsize 1\n",
            "129 | INFO | EPOCH 20 - PROGRESS: at 82.67% examples, 90278 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 20 - PROGRESS: at 89.54% examples, 88012 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 20 - PROGRESS: at 97.88% examples, 87439 words/s, in_qsize 10, out_qsize 0\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 20 : training on 1206392 raw words (929003 effective words) took 10.3s, 90471 effective words/s\n",
            "129 | INFO | EPOCH 21 - PROGRESS: at 10.47% examples, 69880 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 21 - PROGRESS: at 27.16% examples, 84570 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 21 - PROGRESS: at 43.82% examples, 88286 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 21 - PROGRESS: at 60.55% examples, 93253 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 21 - PROGRESS: at 71.29% examples, 90436 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 21 - PROGRESS: at 76.71% examples, 90100 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 21 - PROGRESS: at 84.12% examples, 87287 words/s, in_qsize 13, out_qsize 2\n",
            "129 | INFO | EPOCH 21 - PROGRESS: at 94.92% examples, 89543 words/s, in_qsize 14, out_qsize 0\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | EPOCH 21 - PROGRESS: at 99.76% examples, 89422 words/s, in_qsize 2, out_qsize 1\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 21 : training on 1206392 raw words (928826 effective words) took 10.2s, 90695 effective words/s\n",
            "129 | INFO | EPOCH 22 - PROGRESS: at 10.47% examples, 60161 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 22 - PROGRESS: at 27.20% examples, 79195 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 22 - PROGRESS: at 43.82% examples, 87393 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 22 - PROGRESS: at 60.48% examples, 90121 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 22 - PROGRESS: at 71.83% examples, 93583 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 22 - PROGRESS: at 75.94% examples, 89233 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 22 - PROGRESS: at 85.63% examples, 89973 words/s, in_qsize 13, out_qsize 2\n",
            "129 | INFO | EPOCH 22 - PROGRESS: at 94.17% examples, 88604 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 22 : training on 1206392 raw words (929003 effective words) took 10.2s, 91138 effective words/s\n",
            "129 | INFO | EPOCH 23 - PROGRESS: at 10.47% examples, 61040 words/s, in_qsize 14, out_qsize 1\n",
            "129 | INFO | EPOCH 23 - PROGRESS: at 27.25% examples, 80873 words/s, in_qsize 15, out_qsize 2\n",
            "129 | INFO | EPOCH 23 - PROGRESS: at 43.81% examples, 85435 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 23 - PROGRESS: at 60.48% examples, 89563 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 23 - PROGRESS: at 71.61% examples, 91056 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 23 - PROGRESS: at 75.94% examples, 88268 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 23 - PROGRESS: at 84.12% examples, 87616 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 23 - PROGRESS: at 94.17% examples, 88196 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | EPOCH 23 - PROGRESS: at 99.50% examples, 87568 words/s, in_qsize 4, out_qsize 1\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 23 : training on 1206392 raw words (929504 effective words) took 10.3s, 89893 effective words/s\n",
            "129 | INFO | EPOCH 24 - PROGRESS: at 10.47% examples, 65300 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 24 - PROGRESS: at 27.16% examples, 84403 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 24 - PROGRESS: at 43.82% examples, 90897 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 24 - PROGRESS: at 56.41% examples, 90679 words/s, in_qsize 16, out_qsize 3\n",
            "129 | INFO | EPOCH 24 - PROGRESS: at 70.60% examples, 92631 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 24 - PROGRESS: at 73.12% examples, 89377 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 24 - PROGRESS: at 82.67% examples, 89524 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 24 - PROGRESS: at 91.07% examples, 87873 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | EPOCH 24 - PROGRESS: at 99.38% examples, 89411 words/s, in_qsize 5, out_qsize 1\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 24 : training on 1206392 raw words (929280 effective words) took 10.2s, 91012 effective words/s\n",
            "129 | INFO | EPOCH 25 - PROGRESS: at 10.50% examples, 61961 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 25 - PROGRESS: at 27.16% examples, 80472 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 25 - PROGRESS: at 43.82% examples, 88051 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 25 - PROGRESS: at 60.48% examples, 90487 words/s, in_qsize 14, out_qsize 1\n",
            "129 | INFO | EPOCH 25 - PROGRESS: at 71.50% examples, 89771 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 25 - PROGRESS: at 76.71% examples, 90302 words/s, in_qsize 16, out_qsize 1\n",
            "129 | INFO | EPOCH 25 - PROGRESS: at 84.12% examples, 87383 words/s, in_qsize 14, out_qsize 1\n",
            "129 | INFO | EPOCH 25 - PROGRESS: at 94.17% examples, 88940 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | EPOCH 25 - PROGRESS: at 99.64% examples, 88884 words/s, in_qsize 3, out_qsize 1\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 25 : training on 1206392 raw words (929058 effective words) took 10.3s, 90630 effective words/s\n",
            "129 | INFO | EPOCH 26 - PROGRESS: at 9.42% examples, 66621 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 26 - PROGRESS: at 26.16% examples, 87300 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 26 - PROGRESS: at 38.61% examples, 88467 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 26 - PROGRESS: at 52.21% examples, 90496 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 26 - PROGRESS: at 66.74% examples, 93497 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 26 - PROGRESS: at 71.97% examples, 90131 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 26 - PROGRESS: at 78.25% examples, 89294 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 26 - PROGRESS: at 86.42% examples, 88304 words/s, in_qsize 14, out_qsize 1\n",
            "129 | INFO | EPOCH 26 - PROGRESS: at 96.45% examples, 86990 words/s, in_qsize 11, out_qsize 1\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 26 : training on 1206392 raw words (929181 effective words) took 10.3s, 90068 effective words/s\n",
            "129 | INFO | EPOCH 27 - PROGRESS: at 9.42% examples, 67019 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 27 - PROGRESS: at 26.16% examples, 87157 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 27 - PROGRESS: at 42.79% examples, 90871 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 27 - PROGRESS: at 59.50% examples, 93624 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 27 - PROGRESS: at 70.52% examples, 90228 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 27 - PROGRESS: at 75.20% examples, 91140 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 27 - PROGRESS: at 83.40% examples, 88667 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 27 - PROGRESS: at 93.37% examples, 89936 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | EPOCH 27 - PROGRESS: at 99.40% examples, 88944 words/s, in_qsize 5, out_qsize 1\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 27 : training on 1206392 raw words (929270 effective words) took 10.2s, 91097 effective words/s\n",
            "129 | INFO | EPOCH 28 - PROGRESS: at 10.47% examples, 64360 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 28 - PROGRESS: at 27.25% examples, 79588 words/s, in_qsize 16, out_qsize 1\n",
            "129 | INFO | EPOCH 28 - PROGRESS: at 43.82% examples, 87279 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 28 - PROGRESS: at 60.48% examples, 90728 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 28 - PROGRESS: at 71.61% examples, 91846 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 28 - PROGRESS: at 75.94% examples, 89817 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 28 - PROGRESS: at 84.86% examples, 89466 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 28 - PROGRESS: at 94.17% examples, 88340 words/s, in_qsize 14, out_qsize 1\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | EPOCH 28 - PROGRESS: at 99.76% examples, 89353 words/s, in_qsize 2, out_qsize 1\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 28 : training on 1206392 raw words (929338 effective words) took 10.3s, 90512 effective words/s\n",
            "129 | INFO | EPOCH 29 - PROGRESS: at 10.47% examples, 59857 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 29 - PROGRESS: at 27.16% examples, 81733 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 29 - PROGRESS: at 43.82% examples, 87958 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 29 - PROGRESS: at 60.48% examples, 92372 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 29 - PROGRESS: at 71.50% examples, 90377 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 29 - PROGRESS: at 75.94% examples, 90248 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 29 - PROGRESS: at 82.67% examples, 87673 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 29 - PROGRESS: at 91.07% examples, 87611 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 29 - PROGRESS: at 99.16% examples, 88132 words/s, in_qsize 7, out_qsize 1\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 29 : training on 1206392 raw words (928917 effective words) took 10.4s, 89431 effective words/s\n",
            "129 | INFO | EPOCH 30 - PROGRESS: at 9.42% examples, 65106 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 30 - PROGRESS: at 24.10% examples, 83720 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 30 - PROGRESS: at 36.55% examples, 83295 words/s, in_qsize 14, out_qsize 1\n",
            "129 | INFO | EPOCH 30 - PROGRESS: at 53.25% examples, 88708 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 30 - PROGRESS: at 67.75% examples, 91957 words/s, in_qsize 14, out_qsize 1\n",
            "129 | INFO | EPOCH 30 - PROGRESS: at 72.21% examples, 89228 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 30 - PROGRESS: at 81.21% examples, 89460 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 30 - PROGRESS: at 89.54% examples, 88238 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 30 - PROGRESS: at 99.03% examples, 88768 words/s, in_qsize 8, out_qsize 0\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 30 : training on 1206392 raw words (929213 effective words) took 10.3s, 90510 effective words/s\n",
            "129 | INFO | EPOCH 31 - PROGRESS: at 9.42% examples, 64092 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 31 - PROGRESS: at 26.16% examples, 80586 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 31 - PROGRESS: at 42.79% examples, 88406 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 31 - PROGRESS: at 59.50% examples, 91838 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 31 - PROGRESS: at 69.82% examples, 88607 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 31 - PROGRESS: at 72.90% examples, 87675 words/s, in_qsize 12, out_qsize 3\n",
            "129 | INFO | EPOCH 31 - PROGRESS: at 82.67% examples, 86904 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 31 - PROGRESS: at 91.07% examples, 85553 words/s, in_qsize 16, out_qsize 4\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | EPOCH 31 - PROGRESS: at 99.40% examples, 87155 words/s, in_qsize 5, out_qsize 1\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 31 : training on 1206392 raw words (929332 effective words) took 10.4s, 88945 effective words/s\n",
            "129 | INFO | EPOCH 32 - PROGRESS: at 9.42% examples, 65280 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 32 - PROGRESS: at 26.16% examples, 84873 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 32 - PROGRESS: at 38.62% examples, 84884 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 32 - PROGRESS: at 53.25% examples, 86942 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 32 - PROGRESS: at 69.82% examples, 89937 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 32 - PROGRESS: at 72.33% examples, 87080 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 32 - PROGRESS: at 82.67% examples, 87830 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 32 - PROGRESS: at 91.07% examples, 86362 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | EPOCH 32 - PROGRESS: at 99.27% examples, 87082 words/s, in_qsize 6, out_qsize 0\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 32 : training on 1206392 raw words (929129 effective words) took 10.4s, 89329 effective words/s\n",
            "129 | INFO | EPOCH 33 - PROGRESS: at 10.50% examples, 62547 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 33 - PROGRESS: at 27.22% examples, 79861 words/s, in_qsize 14, out_qsize 1\n",
            "129 | INFO | EPOCH 33 - PROGRESS: at 43.82% examples, 86237 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 33 - PROGRESS: at 60.55% examples, 88790 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 33 - PROGRESS: at 71.61% examples, 89814 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 33 - PROGRESS: at 75.94% examples, 88215 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 33 - PROGRESS: at 84.86% examples, 88330 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 33 - PROGRESS: at 94.17% examples, 87289 words/s, in_qsize 15, out_qsize 1\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | EPOCH 33 - PROGRESS: at 100.00% examples, 89804 words/s, in_qsize 0, out_qsize 1\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 33 : training on 1206392 raw words (929035 effective words) took 10.3s, 89783 effective words/s\n",
            "129 | INFO | EPOCH 34 - PROGRESS: at 9.42% examples, 65838 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 34 - PROGRESS: at 26.16% examples, 83136 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 34 - PROGRESS: at 42.79% examples, 89684 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 34 - PROGRESS: at 58.46% examples, 94635 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 34 - PROGRESS: at 69.82% examples, 90937 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 34 - PROGRESS: at 74.45% examples, 91605 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 34 - PROGRESS: at 81.21% examples, 88685 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 34 - PROGRESS: at 88.76% examples, 87312 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 34 - PROGRESS: at 99.03% examples, 87826 words/s, in_qsize 8, out_qsize 0\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 34 : training on 1206392 raw words (929029 effective words) took 10.3s, 89784 effective words/s\n",
            "129 | INFO | EPOCH 35 - PROGRESS: at 10.47% examples, 74757 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 35 - PROGRESS: at 20.96% examples, 75223 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 35 - PROGRESS: at 36.52% examples, 85667 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 35 - PROGRESS: at 53.25% examples, 89675 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 35 - PROGRESS: at 69.82% examples, 91686 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 35 - PROGRESS: at 72.78% examples, 87285 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 35 - PROGRESS: at 82.67% examples, 89446 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 35 - PROGRESS: at 90.31% examples, 86639 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | EPOCH 35 - PROGRESS: at 99.27% examples, 88646 words/s, in_qsize 6, out_qsize 1\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 35 : training on 1206392 raw words (928852 effective words) took 10.3s, 90090 effective words/s\n",
            "129 | INFO | EPOCH 36 - PROGRESS: at 9.42% examples, 67013 words/s, in_qsize 14, out_qsize 1\n",
            "129 | INFO | EPOCH 36 - PROGRESS: at 24.08% examples, 86658 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 36 - PROGRESS: at 36.52% examples, 87499 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 36 - PROGRESS: at 51.13% examples, 89347 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 36 - PROGRESS: at 67.75% examples, 91200 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 36 - PROGRESS: at 72.21% examples, 89390 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 36 - PROGRESS: at 81.21% examples, 88304 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 36 - PROGRESS: at 90.30% examples, 88090 words/s, in_qsize 14, out_qsize 1\n",
            "129 | INFO | EPOCH 36 - PROGRESS: at 99.03% examples, 87067 words/s, in_qsize 8, out_qsize 0\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 36 : training on 1206392 raw words (929143 effective words) took 10.3s, 90242 effective words/s\n",
            "129 | INFO | EPOCH 37 - PROGRESS: at 10.50% examples, 62638 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 37 - PROGRESS: at 27.22% examples, 81977 words/s, in_qsize 15, out_qsize 1\n",
            "129 | INFO | EPOCH 37 - PROGRESS: at 43.82% examples, 88073 words/s, in_qsize 14, out_qsize 1\n",
            "129 | INFO | EPOCH 37 - PROGRESS: at 60.55% examples, 90692 words/s, in_qsize 14, out_qsize 1\n",
            "129 | INFO | EPOCH 37 - PROGRESS: at 71.61% examples, 91999 words/s, in_qsize 14, out_qsize 1\n",
            "129 | INFO | EPOCH 37 - PROGRESS: at 75.94% examples, 89263 words/s, in_qsize 14, out_qsize 1\n",
            "129 | INFO | EPOCH 37 - PROGRESS: at 85.63% examples, 90156 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 37 - PROGRESS: at 94.17% examples, 88422 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 37 : training on 1206392 raw words (929648 effective words) took 10.2s, 91283 effective words/s\n",
            "129 | INFO | EPOCH 38 - PROGRESS: at 10.47% examples, 63927 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 38 - PROGRESS: at 27.16% examples, 86148 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 38 - PROGRESS: at 41.79% examples, 92161 words/s, in_qsize 12, out_qsize 0\n",
            "129 | INFO | EPOCH 38 - PROGRESS: at 54.32% examples, 88893 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 38 - PROGRESS: at 70.60% examples, 89534 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 38 - PROGRESS: at 75.20% examples, 90497 words/s, in_qsize 14, out_qsize 1\n",
            "129 | INFO | EPOCH 38 - PROGRESS: at 83.40% examples, 87199 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 38 - PROGRESS: at 93.37% examples, 88954 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | EPOCH 38 - PROGRESS: at 99.38% examples, 87734 words/s, in_qsize 5, out_qsize 1\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 38 : training on 1206392 raw words (929077 effective words) took 10.3s, 90468 effective words/s\n",
            "129 | INFO | EPOCH 39 - PROGRESS: at 10.50% examples, 60299 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 39 - PROGRESS: at 27.20% examples, 86954 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 39 - PROGRESS: at 39.69% examples, 87688 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 39 - PROGRESS: at 54.32% examples, 90139 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 39 - PROGRESS: at 70.60% examples, 92269 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 39 - PROGRESS: at 72.90% examples, 88673 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 39 - PROGRESS: at 83.40% examples, 90708 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 39 - PROGRESS: at 91.07% examples, 87165 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 39 - PROGRESS: at 99.16% examples, 87545 words/s, in_qsize 7, out_qsize 1\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 39 : training on 1206392 raw words (929787 effective words) took 10.3s, 90334 effective words/s\n",
            "129 | INFO | EPOCH 40 - PROGRESS: at 9.42% examples, 66050 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 40 - PROGRESS: at 25.13% examples, 89548 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 40 - PROGRESS: at 36.52% examples, 87470 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 40 - PROGRESS: at 51.13% examples, 91809 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 40 - PROGRESS: at 64.66% examples, 92476 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 40 - PROGRESS: at 71.85% examples, 89394 words/s, in_qsize 13, out_qsize 2\n",
            "129 | INFO | EPOCH 40 - PROGRESS: at 79.01% examples, 90290 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 40 - PROGRESS: at 87.21% examples, 88729 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 40 - PROGRESS: at 96.45% examples, 89177 words/s, in_qsize 12, out_qsize 0\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 40 : training on 1206392 raw words (929403 effective words) took 10.2s, 90805 effective words/s\n",
            "129 | INFO | EPOCH 41 - PROGRESS: at 9.42% examples, 61684 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 41 - PROGRESS: at 26.16% examples, 80405 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 41 - PROGRESS: at 42.79% examples, 89177 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 41 - PROGRESS: at 56.33% examples, 90722 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 41 - PROGRESS: at 70.60% examples, 93445 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 41 - PROGRESS: at 72.90% examples, 88513 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 41 - PROGRESS: at 81.94% examples, 88704 words/s, in_qsize 16, out_qsize 2\n",
            "129 | INFO | EPOCH 41 - PROGRESS: at 91.07% examples, 87933 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 41 - PROGRESS: at 99.16% examples, 88094 words/s, in_qsize 7, out_qsize 1\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 41 : training on 1206392 raw words (928944 effective words) took 10.3s, 90545 effective words/s\n",
            "129 | INFO | EPOCH 42 - PROGRESS: at 10.47% examples, 66953 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 42 - PROGRESS: at 26.16% examples, 88087 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 42 - PROGRESS: at 37.56% examples, 86161 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 42 - PROGRESS: at 52.21% examples, 90817 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 42 - PROGRESS: at 68.79% examples, 92975 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 42 - PROGRESS: at 72.21% examples, 89724 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 42 - PROGRESS: at 79.75% examples, 88763 words/s, in_qsize 13, out_qsize 2\n",
            "129 | INFO | EPOCH 42 - PROGRESS: at 88.76% examples, 89315 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 42 - PROGRESS: at 97.20% examples, 88948 words/s, in_qsize 11, out_qsize 0\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 42 : training on 1206392 raw words (929272 effective words) took 10.3s, 90402 effective words/s\n",
            "129 | INFO | EPOCH 43 - PROGRESS: at 9.43% examples, 67438 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 43 - PROGRESS: at 22.04% examples, 79181 words/s, in_qsize 16, out_qsize 2\n",
            "129 | INFO | EPOCH 43 - PROGRESS: at 36.52% examples, 85818 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 43 - PROGRESS: at 53.25% examples, 92308 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 43 - PROGRESS: at 64.66% examples, 89252 words/s, in_qsize 16, out_qsize 2\n",
            "129 | INFO | EPOCH 43 - PROGRESS: at 72.21% examples, 90883 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 43 - PROGRESS: at 79.01% examples, 87451 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 43 - PROGRESS: at 90.31% examples, 89227 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 43 - PROGRESS: at 97.88% examples, 87866 words/s, in_qsize 10, out_qsize 0\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 43 : training on 1206392 raw words (929309 effective words) took 10.3s, 90423 effective words/s\n",
            "129 | INFO | EPOCH 44 - PROGRESS: at 9.42% examples, 64596 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 44 - PROGRESS: at 26.16% examples, 85603 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 44 - PROGRESS: at 38.61% examples, 87467 words/s, in_qsize 15, out_qsize 2\n",
            "129 | INFO | EPOCH 44 - PROGRESS: at 53.25% examples, 88830 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 44 - PROGRESS: at 69.82% examples, 93450 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 44 - PROGRESS: at 72.66% examples, 88435 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 44 - PROGRESS: at 82.67% examples, 91115 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 44 - PROGRESS: at 89.54% examples, 88542 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 44 - PROGRESS: at 97.86% examples, 88088 words/s, in_qsize 10, out_qsize 0\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 44 : training on 1206392 raw words (929372 effective words) took 10.2s, 90843 effective words/s\n",
            "129 | INFO | EPOCH 45 - PROGRESS: at 9.42% examples, 64361 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 45 - PROGRESS: at 26.16% examples, 85768 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 45 - PROGRESS: at 38.61% examples, 87180 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 45 - PROGRESS: at 54.32% examples, 89320 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 45 - PROGRESS: at 70.60% examples, 90650 words/s, in_qsize 15, out_qsize 1\n",
            "129 | INFO | EPOCH 45 - PROGRESS: at 74.45% examples, 89948 words/s, in_qsize 14, out_qsize 1\n",
            "129 | INFO | EPOCH 45 - PROGRESS: at 83.40% examples, 88964 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 45 - PROGRESS: at 91.83% examples, 88392 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | EPOCH 45 - PROGRESS: at 99.38% examples, 88358 words/s, in_qsize 5, out_qsize 1\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 45 : training on 1206392 raw words (929006 effective words) took 10.3s, 90415 effective words/s\n",
            "129 | INFO | EPOCH 46 - PROGRESS: at 9.42% examples, 59719 words/s, in_qsize 14, out_qsize 1\n",
            "129 | INFO | EPOCH 46 - PROGRESS: at 26.16% examples, 85458 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 46 - PROGRESS: at 39.66% examples, 87120 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 46 - PROGRESS: at 54.32% examples, 90449 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 46 - PROGRESS: at 68.79% examples, 91616 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 46 - PROGRESS: at 72.21% examples, 89112 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 46 - PROGRESS: at 79.75% examples, 87748 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 46 - PROGRESS: at 89.54% examples, 88350 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 46 - PROGRESS: at 97.86% examples, 87346 words/s, in_qsize 10, out_qsize 0\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 46 : training on 1206392 raw words (929583 effective words) took 10.3s, 90112 effective words/s\n",
            "129 | INFO | EPOCH 47 - PROGRESS: at 9.43% examples, 60393 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 47 - PROGRESS: at 26.16% examples, 86115 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 47 - PROGRESS: at 39.69% examples, 87086 words/s, in_qsize 13, out_qsize 2\n",
            "129 | INFO | EPOCH 47 - PROGRESS: at 53.28% examples, 87987 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 47 - PROGRESS: at 69.82% examples, 89950 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 47 - PROGRESS: at 73.66% examples, 89834 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 47 - PROGRESS: at 81.21% examples, 88323 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 47 - PROGRESS: at 90.31% examples, 87134 words/s, in_qsize 13, out_qsize 2\n",
            "129 | INFO | EPOCH 47 - PROGRESS: at 99.03% examples, 87815 words/s, in_qsize 8, out_qsize 0\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 47 : training on 1206392 raw words (929183 effective words) took 10.4s, 89572 effective words/s\n",
            "129 | INFO | EPOCH 48 - PROGRESS: at 10.47% examples, 62375 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 48 - PROGRESS: at 26.16% examples, 85481 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 48 - PROGRESS: at 39.66% examples, 88775 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 48 - PROGRESS: at 54.32% examples, 90231 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 48 - PROGRESS: at 68.79% examples, 92195 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 48 - PROGRESS: at 72.21% examples, 89374 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 48 - PROGRESS: at 81.94% examples, 90118 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 48 - PROGRESS: at 89.54% examples, 88507 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 48 - PROGRESS: at 99.03% examples, 89907 words/s, in_qsize 8, out_qsize 0\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 48 : training on 1206392 raw words (929156 effective words) took 10.3s, 90370 effective words/s\n",
            "129 | INFO | EPOCH 49 - PROGRESS: at 9.43% examples, 60120 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 49 - PROGRESS: at 26.16% examples, 78178 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 49 - PROGRESS: at 41.79% examples, 88608 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 49 - PROGRESS: at 53.25% examples, 86029 words/s, in_qsize 14, out_qsize 1\n",
            "129 | INFO | EPOCH 49 - PROGRESS: at 67.75% examples, 86913 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 49 - PROGRESS: at 72.21% examples, 86338 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 49 - PROGRESS: at 80.48% examples, 87190 words/s, in_qsize 16, out_qsize 0\n",
            "129 | INFO | EPOCH 49 - PROGRESS: at 87.21% examples, 83413 words/s, in_qsize 14, out_qsize 1\n",
            "129 | INFO | EPOCH 49 - PROGRESS: at 97.86% examples, 85616 words/s, in_qsize 9, out_qsize 1\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 49 : training on 1206392 raw words (929541 effective words) took 10.8s, 86442 effective words/s\n",
            "129 | INFO | EPOCH 50 - PROGRESS: at 9.43% examples, 57868 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 50 - PROGRESS: at 25.13% examples, 83573 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 50 - PROGRESS: at 35.53% examples, 79698 words/s, in_qsize 14, out_qsize 1\n",
            "129 | INFO | EPOCH 50 - PROGRESS: at 51.13% examples, 84595 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 50 - PROGRESS: at 67.75% examples, 89237 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 50 - PROGRESS: at 71.97% examples, 84162 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 50 - PROGRESS: at 79.75% examples, 85782 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 50 - PROGRESS: at 87.97% examples, 83867 words/s, in_qsize 15, out_qsize 0\n",
            "129 | INFO | EPOCH 50 - PROGRESS: at 96.45% examples, 83236 words/s, in_qsize 11, out_qsize 1\n",
            "129 | INFO | worker thread finished; awaiting finish of 7 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 6 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 5 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 4 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 3 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 2 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 1 more threads\n",
            "129 | INFO | worker thread finished; awaiting finish of 0 more threads\n",
            "129 | INFO | EPOCH - 50 : training on 1206392 raw words (929968 effective words) took 10.8s, 85779 effective words/s\n",
            "129 | INFO | training on a 60319600 raw words (46458384 effective words) took 517.6s, 89764 effective words/s\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-jCCPHrIFt6",
        "colab_type": "code",
        "outputId": "2d430fc7-98c3-44ea-ea79-6703384dd2fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(all_word_model_ft.wv.vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37350"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UloA8JQaINQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "data_with_tweet = copy.deepcopy(x2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6g0bjByMIPAY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "# model.cuda();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3BRh_Q0IQlv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6x8e3cuISId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "x3 = copy.deepcopy(x2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtWvsNlKIUmr",
        "colab_type": "code",
        "outputId": "2429baab-5929-4875-c838-dadb6025ef1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "x2['isvalid'] = False\n",
        "validation['isvalid'] = True\n",
        "all_data = pd.concat([x2,validation], axis=0)\n",
        "print(all_data.shape)\n",
        "all_data.head(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(12859, 5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "      <th>label_encoded</th>\n",
              "      <th>isvalid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>lost vision</td>\n",
              "      <td>10047522</td>\n",
              "      <td>cymbalta withdrawal has reached a peak , lost ...</td>\n",
              "      <td>388</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>loss vision things occur</td>\n",
              "      <td>10047522</td>\n",
              "      <td></td>\n",
              "      <td>388</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       text     label  ... label_encoded  isvalid\n",
              "0               lost vision  10047522  ...           388    False\n",
              "1  loss vision things occur  10047522  ...           388    False\n",
              "\n",
              "[2 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBrKHfLkIWNp",
        "colab_type": "code",
        "outputId": "101a859c-1df6-4845-e1b6-621add7da264",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import unicodedata\n",
        "import string\n",
        "\n",
        "all_letters = string.ascii_lowercase + \" .,0123456789\"\n",
        "n_letters = len(all_letters)\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "  if isNotNan(s):\n",
        "    s = s.lower().strip()\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in all_letters\n",
        "    )\n",
        "\n",
        "category_lines = {}\n",
        "all_categories = []\n",
        "\n",
        "all_data['cleansed_text'] = all_data['text'].apply(lambda x: unicodeToAscii(x))\n",
        "all_data['cleansed_tweet'] = all_data['tweet'].apply(lambda x: unicodeToAscii(x))\n",
        "\n",
        "print(all_letters)\n",
        "print(n_letters)\n",
        "\n",
        "import torch\n",
        "\n",
        "# Find letter index from all_letters, e.g. \"a\" = 0\n",
        "def letterToIndex(letter):\n",
        "    return all_letters.find(letter)\n",
        "\n",
        "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
        "def letterToTensor(letter):\n",
        "    tensor = torch.zeros(1, n_letters)\n",
        "    tensor[0][letterToIndex(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "# Turn a line into a <line_length x 1 x n_letters>,\n",
        "# or an array of one-hot letter vectors\n",
        "def lineToTensor(line):\n",
        "    tensor = torch.zeros(len(line), 1, n_letters)\n",
        "    for li, letter in enumerate(line):\n",
        "        tensor[li][0][letterToIndex(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "print(letterToTensor('4'))\n",
        "\n",
        "print(lineToTensor('withdraw 3 pm').size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "abcdefghijklmnopqrstuvwxyz .,0123456789\n",
            "39\n",
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "         0., 0., 0.]])\n",
            "torch.Size([13, 1, 39])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgkXFKuLIhO4",
        "colab_type": "code",
        "outputId": "7a010b84-2e64-45e9-c033-5636660d12b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "lengths = [len(i) for i in list(all_data['cleansed_text'])]\n",
        "max(lengths), min(lengths)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(111, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPxZ3PLCIhsr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars = []\n",
        "for i in list(all_data['cleansed_text']):\n",
        "  chars.extend(list(i))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MRo7J15IjgO",
        "colab_type": "code",
        "outputId": "3f3ab6fa-4c91-4c85-f1f0-946ef8df3730",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "char2idx = {}\n",
        "idx2char = {}\n",
        "char2idx[''] = 0\n",
        "idx2char[0] = ''\n",
        "\n",
        "for ix, char in enumerate(list(set(chars))):\n",
        "  char2idx[char] = ix+1\n",
        "  idx2char[ix+1] = char\n",
        "\n",
        "word2idx = {}\n",
        "idx2word = {}\n",
        "words = []\n",
        "word2idx[''] = 0\n",
        "idx2word[0] = ''\n",
        "\n",
        "for i in list(all_tweets['tweet']):\n",
        "  words.extend(i.split())\n",
        "\n",
        "for ix, wrd in enumerate(list(set(words))):\n",
        "  word2idx[wrd] = ix+1\n",
        "  idx2word[ix+1] = wrd\n",
        "\n",
        "token2idx = {}\n",
        "idx2token = {}\n",
        "\n",
        "for ix, token in enumerate(list(set(words + chars))):\n",
        "  token2idx[token] = ix\n",
        "  idx2token[ix] = token\n",
        "\n",
        "print(len(char2idx), len(word2idx), len(token2idx))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "39 37351 37351\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnaac_RDIzib",
        "colab_type": "code",
        "outputId": "5c7b511a-6f65-4895-f449-79af0ed71b8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "train_char_input = []\n",
        "test_char_input = []\n",
        "train_word_input = []\n",
        "test_word_input = []\n",
        "train_output = []\n",
        "test_output = []\n",
        "not_found_word_list = []\n",
        "for ix, row in all_data.iterrows():\n",
        "\n",
        "  text = row['cleansed_text']\n",
        "  tweet = row['cleansed_tweet']\n",
        "  label = row['label_encoded']\n",
        "  text_seq = [char2idx[i] for i in list(text)]\n",
        "  tweet_seq = []\n",
        "  if tweet == '':\n",
        "    tweet_seq.append(word2idx[tweet])\n",
        "  else:\n",
        "    for i in tweet.split():\n",
        "      try:\n",
        "        tweet_seq.append(word2idx[i])\n",
        "      except:\n",
        "        most_sim = all_word_model_ft.wv.most_similar(i)[0][0]\n",
        "        tweet_seq.append(word2idx[most_sim])\n",
        "        not_found_word_list.append([i, most_sim])\n",
        "\n",
        "  if row['isvalid'] == False:\n",
        "    train_char_input.append(torch.tensor(text_seq))\n",
        "    train_word_input.append(torch.tensor(tweet_seq))\n",
        "    train_output.append(label)\n",
        "  else:\n",
        "    test_char_input.append(torch.tensor(text_seq))\n",
        "    test_word_input.append(torch.tensor(tweet_seq))\n",
        "    test_output.append(label)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "129 | INFO | precomputing L2-norms of word weight vectors\n",
            "129 | INFO | precomputing L2-norms of ngram weight vectors\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xyj25V43NZnV",
        "colab_type": "text"
      },
      "source": [
        "CREATE DATA LOADERS FOR TESTING SET:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RywAfLc1I1sJ",
        "colab_type": "code",
        "outputId": "0613ef52-e08f-4a31-a9c5-cc714cc584a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(not_found_word_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOIbSDqXI2XV",
        "colab_type": "code",
        "outputId": "e7f7bb58-fab7-416d-d64a-a3f8e26f6f62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train_char = pad_sequence(train_char_input, batch_first=True)\n",
        "x_train_word = pad_sequence(train_word_input, batch_first=True)\n",
        "y_train = torch.tensor(train_output)\n",
        "\n",
        "x_dev_char = pad_sequence(test_char_input, batch_first=True)\n",
        "x_dev_word = pad_sequence(test_word_input, batch_first=True)\n",
        "y_dev = torch.tensor(test_output)\n",
        "\n",
        "\n",
        "print(x_train_char.shape, x_train_word.shape, y_train.shape, x_dev_char.shape, x_dev_word.shape, y_dev.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([12494, 111]) torch.Size([12494, 760]) torch.Size([12494]) torch.Size([365, 43]) torch.Size([365, 31]) torch.Size([365])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ev3lnUgVI4U8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating embedding weight matrix\n",
        "matrix_len = len(word2idx)\n",
        "weights_matrix = np.zeros((matrix_len, 200))\n",
        "words_found = 0\n",
        "\n",
        "i = 0\n",
        "for k,v in word2idx.items():\n",
        "  if k == '':\n",
        "    weights_matrix[i] = np.random.rand(200)\n",
        "  else:\n",
        "    weights_matrix[i] = all_word_model_ft.wv.get_vector(k)\n",
        "  i += 1  \n",
        "pickle.dump(weights_matrix, open('smm4h_task3_word_embedding_weights_matrix.pickle', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2A-PU-YNI8LJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights_matrix = pickle.load(open('smm4h_task3_word_embedding_weights_matrix.pickle', 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObJkOQoTI-E1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_emb_layer(weights_matrix, non_trainable=False):\n",
        "  num_embeddings, embedding_dim = weights_matrix.shape\n",
        "  emb_layer = nn.Embedding.from_pretrained(torch.tensor(weights_matrix), freeze = non_trainable)\n",
        "  return emb_layer, embedding_dim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC2bgcObI_SL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.optim import Adam\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "bs = 128\n",
        "\n",
        "train_data = TensorDataset(x_train_char, x_train_word, y_train)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
        "\n",
        "valid_data = TensorDataset(x_dev_char, x_dev_word, y_dev)\n",
        "valid_sampler = SequentialSampler(valid_data)\n",
        "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIh54g-hJArA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, char_vocab_size, \n",
        "                tagset_size, n_layers=2, dropout=0.5,\n",
        "                 weights_matrix=weights_matrix):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.char_embeddings = nn.Embedding(char_vocab_size, embedding_dim)\n",
        "        self.word_embeddings, self.word_embeddings_size = create_emb_layer(weights_matrix)\n",
        "        #nn.Embedding(word_vocab_size, embedding_dim)\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "        self.drp = nn.Dropout(self.dropout)\n",
        "        self.context_gru = nn.GRU(self.word_embeddings_size, hidden_dim, bidirectional=True, \n",
        "                            num_layers=self.n_layers,\n",
        "                            dropout=self.dropout)\n",
        "        \n",
        "        self.char_gru = nn.GRU(embedding_dim*2, hidden_dim*2, bidirectional=True, \n",
        "                            num_layers=self.n_layers,\n",
        "                            dropout=self.dropout)\n",
        "        self.hidden2tag = nn.Linear(hidden_dim*4, tagset_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, char, word):\n",
        "        char_embeds = self.char_embeddings(char)\n",
        "        char_embeds = self.drp(char_embeds)\n",
        "\n",
        "        word_embeds = self.word_embeddings(word)\n",
        "        word_embeds = self.drp(word_embeds)\n",
        "\n",
        "        # print(char_embeds.shape, word_embeds.shape)\n",
        "        word_embeds = word_embeds.float()\n",
        "        word_embeds = word_embeds.to(device)\n",
        "        # return word_embeds, char_embeds\n",
        "        _,tweet_context = self.context_gru(word_embeds)\n",
        "        # print(tweet_context.shape, char_embeds.shape)\n",
        "        \n",
        "        tweet_context = tweet_context.sum(axis=0)\n",
        "        char_embeds = torch.cat([char_embeds, \n",
        "                                 tweet_context.repeat(char_embeds.shape[0], 1,1)], -1)\n",
        "        # contextual_inputs = torch.cat([char_embeds, tweet_context])\n",
        "        _, hidden = self.char_gru(char_embeds)\n",
        "        # print(char_embeds.shape, hidden.shape)\n",
        "        hidden = hidden.view(self.n_layers,2,-1,self.hidden_dim*2)\n",
        "        # print(hidden.shape)\n",
        "        hidden = hidden.sum(axis=0)\n",
        "        # print(hidden.shape)\n",
        "        hidden = torch.cat([hidden[:1,:,:], hidden[1:,:,:]], dim=-1).squeeze(0)\n",
        "        # print(hidden.shape)\n",
        "        hidden = self.drp(hidden)\n",
        "        tag_space = self.hidden2tag(hidden)\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZCOrc14JDih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 256\n",
        "hidden_dim = 256\n",
        "char_vocab_size = len(char2idx)\n",
        "# word_vocab_size = len(word2idx)\n",
        "tagset_size = len(set(list(all_data['label_encoded'])))\n",
        "n_layers = 2\n",
        "dropout = 0.4\n",
        "weights_matrix = weights_matrix\n",
        "lstm = RNN(embedding_dim, hidden_dim, char_vocab_size, tagset_size,\n",
        "           n_layers, dropout, weights_matrix)\n",
        "lstm = lstm.float() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfYGly_sJGQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm = lstm.float() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6HVTpacJHHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 3e-4\n",
        "loss_fct_classification = torch.nn.CrossEntropyLoss()\n",
        "optimizer = Adam(lstm.parameters(), lr = lr)\n",
        "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=lr, max_lr=lr*10, \n",
        "                                              cycle_momentum=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lXzv-sZJOxs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm.cuda();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKaG-HnDJQdU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flat_accuracy_classification(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2CnSYovJUbq",
        "colab_type": "code",
        "outputId": "1d3c542a-5029-424f-e827-e75df923fc25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score,confusion_matrix\n",
        "\n",
        "epochs = 25\n",
        "max_grad_norm = 1.0\n",
        "lstm.zero_grad()\n",
        "best_f1 = 0\n",
        "stats = {}\n",
        "for ep in tqdm(range(epochs)):\n",
        "  # TRAIN loop\n",
        "    lstm.train()\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    for batch in train_dataloader:\n",
        "\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      x_train_char, x_train_word, y_train = batch\n",
        "      x_train_char = x_train_char.T\n",
        "      x_train_word = x_train_word.T\n",
        "      outputs = lstm(x_train_char, x_train_word)\n",
        "      \n",
        "      loss = loss_fct_classification(outputs, y_train)\n",
        "      \n",
        "      loss.backward()\n",
        "\n",
        "      tr_loss += loss.item()\n",
        "      nb_tr_examples += x_train_char.size(1)\n",
        "      nb_tr_steps += 1\n",
        "\n",
        "      torch.nn.utils.clip_grad_norm_(parameters=lstm.parameters(), \n",
        "                                     max_norm=max_grad_norm)\n",
        "\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "      lstm.zero_grad()\n",
        "\n",
        "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "    stats[ep] = {} \n",
        "    stats[ep]['train_loss'] = tr_loss/nb_tr_steps\n",
        "    # VALIDATION on validation set\n",
        "    lstm.eval()\n",
        "    eval_loss, eval_accuracy_class = 0, 0\n",
        "    \n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    class_preds ,class_true_labels = [], []\n",
        "    \n",
        "\n",
        "    for batch in valid_dataloader:\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      x_dev_char, x_dev_word, y_dev = batch\n",
        "      x_dev_char = x_dev_char.T\n",
        "      x_dev_word = x_dev_word.T\n",
        "  \n",
        "      with torch.no_grad():\n",
        "        outputs = lstm(x_dev_char, x_dev_word)\n",
        "        eval_loss = loss_fct_classification(outputs, y_dev)\n",
        "\n",
        "      classification_logits = outputs.detach().cpu().numpy()\n",
        "      class_preds.extend(list(np.argmax(classification_logits,axis=1)))\n",
        "      #print(len(class_preds))\n",
        "      \n",
        "      class_label_ids = y_dev.to('cpu').numpy()\n",
        "      \n",
        "      class_true_labels.append(class_label_ids)\n",
        "      \n",
        "      tmp_eval_accuracy_class = flat_accuracy_classification(classification_logits, \n",
        "                                              class_label_ids)\n",
        "      eval_loss += eval_loss.mean().item()\n",
        "\n",
        "      eval_accuracy_class += tmp_eval_accuracy_class\n",
        "      nb_eval_examples += x_dev_char.size(1)\n",
        "      nb_eval_steps += 1\n",
        "    eval_loss = eval_loss/nb_eval_steps\n",
        "    stats[ep]['eval_loss'] = eval_loss\n",
        "    print(\"Validation loss: {}\".format(eval_loss))\n",
        "    print(\"Validation Accuracy Classifier: {}\".format(eval_accuracy_class/nb_eval_steps))\n",
        "    valid_tags = [l_i for l in class_true_labels for l_i in l ]\n",
        "    pred_tags = [p for p in class_preds]\n",
        "    print(\"Val Micro F1 \",f1_score(valid_tags, pred_tags, average='micro'))\n",
        "    print(\"Val Micro Precision \",precision_score(valid_tags, pred_tags, average='micro'))\n",
        "    print(\"Val Micro Recall \",recall_score(valid_tags, pred_tags, average='micro'))\n",
        "    print(\"Val Macro F1 \",f1_score(valid_tags, pred_tags, average='macro'))\n",
        "    print(\"Val Macro Precision \",precision_score(valid_tags, pred_tags, average='macro'))\n",
        "    print(\"Val Macro Recall \",recall_score(valid_tags, pred_tags, average='macro'))\n",
        "    print(\"Val Accuracy \",accuracy_score(valid_tags, pred_tags))\n",
        "    print(\"\\n\")\n",
        "    # print(\"Confusion matrix \\n\", confusion_matrix(valid_tags, pred_tags))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/25 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "  4%|▍         | 1/25 [01:24<33:59, 84.99s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 5.52839428551343\n",
            "Validation loss: 4.066431999206543\n",
            "Validation Accuracy Classifier: 0.14086391437308868\n",
            "Val Micro F1  0.14246575342465753\n",
            "Val Micro Precision  0.14246575342465753\n",
            "Val Micro Recall  0.14246575342465753\n",
            "Val Macro F1  0.052162852169044115\n",
            "Val Macro Precision  0.046973941707923406\n",
            "Val Macro Recall  0.10176691729323308\n",
            "Val Accuracy  0.14246575342465753\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  8%|▊         | 2/25 [02:49<32:34, 84.99s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 4.3853673010456315\n",
            "Validation loss: 3.511462688446045\n",
            "Validation Accuracy Classifier: 0.25262805810397554\n",
            "Val Micro F1  0.2547945205479452\n",
            "Val Micro Precision  0.2547945205479452\n",
            "Val Micro Recall  0.2547945205479452\n",
            "Val Macro F1  0.1314760718323062\n",
            "Val Macro Precision  0.12285738163645141\n",
            "Val Macro Recall  0.1826467331118494\n",
            "Val Accuracy  0.2547945205479452\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 12%|█▏        | 3/25 [04:14<31:06, 84.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 3.338210565703256\n",
            "Validation loss: 2.9639785289764404\n",
            "Validation Accuracy Classifier: 0.33698872324159024\n",
            "Val Micro F1  0.33972602739726027\n",
            "Val Micro Precision  0.33972602739726027\n",
            "Val Micro Recall  0.33972602739726027\n",
            "Val Macro F1  0.20447005917528965\n",
            "Val Macro Precision  0.19743128384837746\n",
            "Val Macro Recall  0.24365781710914453\n",
            "Val Accuracy  0.33972602739726027\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 16%|█▌        | 4/25 [05:39<29:40, 84.80s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 2.4845281656907527\n",
            "Validation loss: 2.9328839778900146\n",
            "Validation Accuracy Classifier: 0.36790424311926606\n",
            "Val Micro F1  0.36986301369863017\n",
            "Val Micro Precision  0.3698630136986301\n",
            "Val Micro Recall  0.3698630136986301\n",
            "Val Macro F1  0.23704021952039256\n",
            "Val Macro Precision  0.2390401576673167\n",
            "Val Macro Recall  0.26998260491411175\n",
            "Val Accuracy  0.3698630136986301\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 5/25 [07:04<28:16, 84.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 1.880933468439141\n",
            "Validation loss: 2.7746214866638184\n",
            "Validation Accuracy Classifier: 0.40142392966360857\n",
            "Val Micro F1  0.40273972602739727\n",
            "Val Micro Precision  0.40273972602739727\n",
            "Val Micro Recall  0.40273972602739727\n",
            "Val Macro F1  0.25444037444037443\n",
            "Val Macro Precision  0.2629396837730171\n",
            "Val Macro Recall  0.28313237688237686\n",
            "Val Accuracy  0.40273972602739727\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 24%|██▍       | 6/25 [08:28<26:51, 84.80s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 1.4770927708976123\n",
            "Validation loss: 2.8873801231384277\n",
            "Validation Accuracy Classifier: 0.40142392966360857\n",
            "Val Micro F1  0.40273972602739727\n",
            "Val Micro Precision  0.40273972602739727\n",
            "Val Micro Recall  0.40273972602739727\n",
            "Val Macro F1  0.2588585679102218\n",
            "Val Macro Precision  0.2604019518493203\n",
            "Val Macro Recall  0.2943609022556391\n",
            "Val Accuracy  0.40273972602739727\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 28%|██▊       | 7/25 [09:53<25:25, 84.74s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 1.1712024370018317\n",
            "Validation loss: 2.8890833854675293\n",
            "Validation Accuracy Classifier: 0.4344896788990826\n",
            "Val Micro F1  0.43561643835616437\n",
            "Val Micro Precision  0.43561643835616437\n",
            "Val Micro Recall  0.43561643835616437\n",
            "Val Macro F1  0.3037533698710447\n",
            "Val Macro Precision  0.31965101323796974\n",
            "Val Macro Recall  0.330406314699793\n",
            "Val Accuracy  0.43561643835616437\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 32%|███▏      | 8/25 [11:18<24:00, 84.72s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.9892307276628456\n",
            "Validation loss: 2.9994053840637207\n",
            "Validation Accuracy Classifier: 0.4061783256880734\n",
            "Val Micro F1  0.40821917808219177\n",
            "Val Micro Precision  0.40821917808219177\n",
            "Val Micro Recall  0.40821917808219177\n",
            "Val Macro F1  0.2761497751316134\n",
            "Val Macro Precision  0.28920096546678825\n",
            "Val Macro Recall  0.29841269841269846\n",
            "Val Accuracy  0.40821917808219177\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 36%|███▌      | 9/25 [12:42<22:34, 84.67s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.837099837405341\n",
            "Validation loss: 3.154961347579956\n",
            "Validation Accuracy Classifier: 0.43618597094801226\n",
            "Val Micro F1  0.4383561643835616\n",
            "Val Micro Precision  0.4383561643835616\n",
            "Val Micro Recall  0.4383561643835616\n",
            "Val Macro F1  0.2895868930704996\n",
            "Val Macro Precision  0.30174180327868855\n",
            "Val Macro Recall  0.31457357533177205\n",
            "Val Accuracy  0.4383561643835616\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 10/25 [14:06<21:08, 84.54s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.775227062252103\n",
            "Validation loss: 3.166179656982422\n",
            "Validation Accuracy Classifier: 0.4236190749235474\n",
            "Val Micro F1  0.42465753424657526\n",
            "Val Micro Precision  0.4246575342465753\n",
            "Val Micro Recall  0.4246575342465753\n",
            "Val Macro F1  0.2749535600053288\n",
            "Val Macro Precision  0.28715846994535515\n",
            "Val Macro Recall  0.2902712724434036\n",
            "Val Accuracy  0.4246575342465753\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 44%|████▍     | 11/25 [15:31<19:42, 84.44s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.7254867860857321\n",
            "Validation loss: 3.4419827461242676\n",
            "Validation Accuracy Classifier: 0.4274655963302752\n",
            "Val Micro F1  0.4301369863013699\n",
            "Val Micro Precision  0.4301369863013699\n",
            "Val Micro Recall  0.4301369863013699\n",
            "Val Macro F1  0.3005195646856108\n",
            "Val Macro Precision  0.3192552407742282\n",
            "Val Macro Recall  0.32087602973678925\n",
            "Val Accuracy  0.4301369863013699\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 48%|████▊     | 12/25 [16:55<18:16, 84.36s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.7190643390830682\n",
            "Validation loss: 3.4328083992004395\n",
            "Validation Accuracy Classifier: 0.4370938455657492\n",
            "Val Micro F1  0.4383561643835616\n",
            "Val Micro Precision  0.4383561643835616\n",
            "Val Micro Recall  0.4383561643835616\n",
            "Val Macro F1  0.30312127331358096\n",
            "Val Macro Precision  0.3212861212861213\n",
            "Val Macro Recall  0.32468457468457473\n",
            "Val Accuracy  0.4383561643835616\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 52%|█████▏    | 13/25 [18:19<16:51, 84.33s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.7248442607874773\n",
            "Validation loss: 3.8493094444274902\n",
            "Validation Accuracy Classifier: 0.4156871177370031\n",
            "Val Micro F1  0.4191780821917808\n",
            "Val Micro Precision  0.4191780821917808\n",
            "Val Micro Recall  0.4191780821917808\n",
            "Val Macro F1  0.2848809696267323\n",
            "Val Macro Precision  0.2960708077233501\n",
            "Val Macro Recall  0.31142050040355124\n",
            "Val Accuracy  0.4191780821917808\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 56%|█████▌    | 14/25 [19:44<15:28, 84.38s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.7572726251519456\n",
            "Validation loss: 3.520531177520752\n",
            "Validation Accuracy Classifier: 0.41535263761467894\n",
            "Val Micro F1  0.4164383561643835\n",
            "Val Micro Precision  0.41643835616438357\n",
            "Val Micro Recall  0.41643835616438357\n",
            "Val Macro F1  0.2858122433122433\n",
            "Val Macro Precision  0.3020121381886087\n",
            "Val Macro Recall  0.3029513888888889\n",
            "Val Accuracy  0.41643835616438357\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 15/25 [21:08<14:03, 84.34s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.8129621224135769\n",
            "Validation loss: 3.7242047786712646\n",
            "Validation Accuracy Classifier: 0.43663990825688076\n",
            "Val Micro F1  0.4383561643835616\n",
            "Val Micro Precision  0.4383561643835616\n",
            "Val Micro Recall  0.4383561643835616\n",
            "Val Macro F1  0.30039425939007525\n",
            "Val Macro Precision  0.3120442319187089\n",
            "Val Macro Recall  0.3259165172345089\n",
            "Val Accuracy  0.4383561643835616\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 64%|██████▍   | 16/25 [22:33<12:40, 84.50s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.8465810287363675\n",
            "Validation loss: 3.963412284851074\n",
            "Validation Accuracy Classifier: 0.4383362003058104\n",
            "Val Micro F1  0.4410958904109589\n",
            "Val Micro Precision  0.4410958904109589\n",
            "Val Micro Recall  0.4410958904109589\n",
            "Val Macro F1  0.29845033939389065\n",
            "Val Macro Precision  0.32149405216632104\n",
            "Val Macro Recall  0.323219287715086\n",
            "Val Accuracy  0.4410958904109589\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 68%|██████▊   | 17/25 [23:57<11:14, 84.34s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.8908681750905757\n",
            "Validation loss: 4.091365814208984\n",
            "Validation Accuracy Classifier: 0.44015194954128445\n",
            "Val Micro F1  0.4410958904109589\n",
            "Val Micro Precision  0.4410958904109589\n",
            "Val Micro Recall  0.4410958904109589\n",
            "Val Macro F1  0.3100080976859018\n",
            "Val Macro Precision  0.32621042919715487\n",
            "Val Macro Recall  0.3404050779603877\n",
            "Val Accuracy  0.4410958904109589\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 72%|███████▏  | 18/25 [25:21<09:50, 84.35s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.9553337954745\n",
            "Validation loss: 4.223069667816162\n",
            "Validation Accuracy Classifier: 0.41535263761467894\n",
            "Val Micro F1  0.4164383561643835\n",
            "Val Micro Precision  0.41643835616438357\n",
            "Val Micro Recall  0.41643835616438357\n",
            "Val Macro F1  0.28221497636593457\n",
            "Val Macro Precision  0.29472573839662447\n",
            "Val Macro Recall  0.3075874020494273\n",
            "Val Accuracy  0.41643835616438357\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 76%|███████▌  | 19/25 [26:46<08:26, 84.43s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 1.027459586153225\n",
            "Validation loss: 3.844388723373413\n",
            "Validation Accuracy Classifier: 0.4357320336391437\n",
            "Val Micro F1  0.4383561643835616\n",
            "Val Micro Precision  0.4383561643835616\n",
            "Val Micro Recall  0.4383561643835616\n",
            "Val Macro F1  0.29451914797593814\n",
            "Val Macro Precision  0.30917048200998815\n",
            "Val Macro Recall  0.31342347638643936\n",
            "Val Accuracy  0.4383561643835616\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 20/25 [28:10<07:01, 84.38s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 1.0574266956168779\n",
            "Validation loss: 4.257530689239502\n",
            "Validation Accuracy Classifier: 0.42916188837920494\n",
            "Val Micro F1  0.4328767123287671\n",
            "Val Micro Precision  0.4328767123287671\n",
            "Val Micro Recall  0.4328767123287671\n",
            "Val Macro F1  0.28587942612942613\n",
            "Val Macro Precision  0.29889610389610394\n",
            "Val Macro Recall  0.31085069444444446\n",
            "Val Accuracy  0.4328767123287671\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 84%|████████▍ | 21/25 [29:35<05:37, 84.48s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 1.1617593686191403\n",
            "Validation loss: 4.186975479125977\n",
            "Validation Accuracy Classifier: 0.417502866972477\n",
            "Val Micro F1  0.4191780821917808\n",
            "Val Micro Precision  0.4191780821917808\n",
            "Val Micro Recall  0.4191780821917808\n",
            "Val Macro F1  0.2856483965988098\n",
            "Val Macro Precision  0.3016036993309721\n",
            "Val Macro Recall  0.3083259543486816\n",
            "Val Accuracy  0.4191780821917808\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 88%|████████▊ | 22/25 [30:59<04:13, 84.57s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 1.1955571241524754\n",
            "Validation loss: 4.141491889953613\n",
            "Validation Accuracy Classifier: 0.4305237003058104\n",
            "Val Micro F1  0.4328767123287671\n",
            "Val Micro Precision  0.4328767123287671\n",
            "Val Micro Recall  0.4328767123287671\n",
            "Val Macro F1  0.29335030982089805\n",
            "Val Macro Precision  0.3040641534391534\n",
            "Val Macro Recall  0.3134747023809524\n",
            "Val Accuracy  0.4328767123287671\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 92%|█████████▏| 23/25 [32:24<02:48, 84.47s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 1.1453494277535652\n",
            "Validation loss: 4.30275297164917\n",
            "Validation Accuracy Classifier: 0.42101490825688076\n",
            "Val Micro F1  0.42191780821917807\n",
            "Val Micro Precision  0.42191780821917807\n",
            "Val Micro Recall  0.42191780821917807\n",
            "Val Macro F1  0.2792666045894166\n",
            "Val Macro Precision  0.2892744225061298\n",
            "Val Macro Recall  0.3002661633759195\n",
            "Val Accuracy  0.42191780821917807\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 96%|█████████▌| 24/25 [33:48<01:24, 84.40s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 1.1313216877835137\n",
            "Validation loss: 4.277045726776123\n",
            "Validation Accuracy Classifier: 0.42316513761467894\n",
            "Val Micro F1  0.42465753424657526\n",
            "Val Micro Precision  0.4246575342465753\n",
            "Val Micro Recall  0.4246575342465753\n",
            "Val Macro F1  0.2710673780001511\n",
            "Val Macro Precision  0.27442476990796316\n",
            "Val Macro Recall  0.2999549819927971\n",
            "Val Accuracy  0.4246575342465753\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 25/25 [35:12<00:00, 84.51s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 1.142112865739939\n",
            "Validation loss: 4.266861915588379\n",
            "Validation Accuracy Classifier: 0.4144447629969419\n",
            "Val Micro F1  0.4164383561643835\n",
            "Val Micro Precision  0.41643835616438357\n",
            "Val Micro Recall  0.41643835616438357\n",
            "Val Macro F1  0.27101210928432196\n",
            "Val Macro Precision  0.2842249452082088\n",
            "Val Macro Recall  0.29284219964136277\n",
            "Val Accuracy  0.41643835616438357\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwIFB1PRKRGH",
        "colab_type": "code",
        "outputId": "9e15330f-374b-4b27-8cf9-d4fc3323825c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "source": [
        "train_losses = []\n",
        "valid_losses = []\n",
        "for k,v in stats.items():\n",
        "  tr = v['train_loss']\n",
        "  vl = v['eval_loss'].detach().cpu().item()\n",
        "  train_losses.append(tr)\n",
        "  valid_losses.append(vl)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(train_losses, label = 'train_loss')\n",
        "plt.plot(valid_losses, label = 'eval_loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iuDxzeU2MxH",
        "colab_type": "code",
        "outputId": "3de7ad41-043c-4a2f-86d0-39ff42836506",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "class_preds = []\n",
        "\n",
        "for batch in valid_dataloader:\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      x_dev_char, x_dev_word, y_dev = batch\n",
        "      x_dev_char = x_dev_char.T\n",
        "      x_dev_word = x_dev_word.T\n",
        "  \n",
        "      with torch.no_grad():\n",
        "        outputs = lstm(x_dev_char, x_dev_word)\n",
        "        #eval_loss = loss_fct_classification(outputs, y_dev)\n",
        "\n",
        "      classification_logits = outputs.detach().cpu().numpy()\n",
        "      class_preds.extend(list(np.argmax(classification_logits,axis=1)))\n",
        "      #print(len(class_preds))\n",
        "\n",
        "hgru = []\n",
        "for i in range(len(class_preds)):\n",
        "  label = numbertolabel(class_preds[i])\n",
        "  hgru.append(label)\n",
        "\n",
        "print(len(hgru))\n",
        "\n",
        "task3mix['hgru_meddra_code'] = hgru"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "365\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HJiHCUNhDVf",
        "colab_type": "code",
        "outputId": "4d10838e-ffbc-4f54-dd4d-c60426bb5b33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 949
        }
      },
      "source": [
        "comp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>extract</th>\n",
              "      <th>type</th>\n",
              "      <th>tweet</th>\n",
              "      <th>cleaned_tweet</th>\n",
              "      <th>meddra_code_cosine</th>\n",
              "      <th>logistic_meddra_code</th>\n",
              "      <th>svc_meddra_code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>332317478170546176</td>\n",
              "      <td>27.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>allergies</td>\n",
              "      <td>ADR</td>\n",
              "      <td>do you have any medication allergies? \"asthma!...</td>\n",
              "      <td>do you have any medication allergies ? asthma ...</td>\n",
              "      <td>10001718</td>\n",
              "      <td>10013661</td>\n",
              "      <td>10013661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>347806215776116737</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>hurt your liver</td>\n",
              "      <td>ADR</td>\n",
              "      <td>@ashleylvivian if #avelox has hurt your liver,...</td>\n",
              "      <td>if a velox has hurt your liver , avoid tylenol...</td>\n",
              "      <td>10024668</td>\n",
              "      <td>10024668</td>\n",
              "      <td>10024668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>350336129817509888</td>\n",
              "      <td>75.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>length of focus today: about 30 seconds</td>\n",
              "      <td>ADR</td>\n",
              "      <td>apparently, baclofen greatly exacerbates the \"...</td>\n",
              "      <td>apparently , baclofen greatly exacerbates the ...</td>\n",
              "      <td>10016356</td>\n",
              "      <td>10047896</td>\n",
              "      <td>10033557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>348644040444637184</td>\n",
              "      <td>26.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>dreams</td>\n",
              "      <td>ADR</td>\n",
              "      <td>@bilgeebiri oh robitussin dreams are notorious...</td>\n",
              "      <td>oh robitussin dreams are notorious although wh...</td>\n",
              "      <td>10079069</td>\n",
              "      <td>10000125</td>\n",
              "      <td>10000125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>352217412046823425</td>\n",
              "      <td>84.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>tendon rupture</td>\n",
              "      <td>ADR</td>\n",
              "      <td>#eds friends! anybody taken #cipro? (antibioti...</td>\n",
              "      <td>eds friends ! anybody taken cipro ? antibiotic...</td>\n",
              "      <td>10043248</td>\n",
              "      <td>10043248</td>\n",
              "      <td>10043248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>434</th>\n",
              "      <td>331289838856835072</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>okay trazodone, work your magic.</td>\n",
              "      <td>okay trazodone , work your magic</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>435</th>\n",
              "      <td>332907977700937731</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>tomorrow, sending mummers her mother's day pos...</td>\n",
              "      <td>tomorrow , sending mummers her mother s day po...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>436</th>\n",
              "      <td>333123450254274560</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@kyleecakes4 i was given like 5mg of abilify a...</td>\n",
              "      <td>i was given like 5 mg of abilify and i still d...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>437</th>\n",
              "      <td>341950988455927808</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>stand down, tweeps! stand down! i did it wrong...</td>\n",
              "      <td>stand down , tweeps ! stand down ! i did it wr...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>438</th>\n",
              "      <td>350709708484644864</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@loveone_another: i'm lowkey pill junkie. &amp;gt;...</td>\n",
              "      <td>i am lowkey pill junkie annoyed vivance , xana...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>439 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               tweet_id  start  ...  logistic_meddra_code svc_meddra_code\n",
              "0    332317478170546176   27.0  ...              10013661        10013661\n",
              "1    347806215776116737   30.0  ...              10024668        10024668\n",
              "2    350336129817509888   75.0  ...              10047896        10033557\n",
              "3    348644040444637184   26.0  ...              10000125        10000125\n",
              "4    352217412046823425   84.0  ...              10043248        10043248\n",
              "..                  ...    ...  ...                   ...             ...\n",
              "434  331289838856835072    NaN  ...                                      \n",
              "435  332907977700937731    NaN  ...                                      \n",
              "436  333123450254274560    NaN  ...                                      \n",
              "437  341950988455927808    NaN  ...                                      \n",
              "438  350709708484644864    NaN  ...                                      \n",
              "\n",
              "[439 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQbUMjyahifm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testingDataFinal = comp\n",
        "testingDataFinal['extract']= testingDataFinal['extract'].apply(lambda x: preprocess(x))\n",
        "testingDataFinal['cleansed_text'] = testingDataFinal['extract'].apply(lambda x: unicodeToAscii(x))\n",
        "testingDataFinal['cleansed_tweet'] = testingDataFinal['cleaned_tweet'].apply(lambda x: unicodeToAscii(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cba_v-B2w4LG",
        "colab_type": "code",
        "outputId": "f852523a-f3ac-430e-d121-3ad56aebc9bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "comp_char_input = []\n",
        "comp_word_input = []\n",
        "comp_output = [] \n",
        "not_found_word_list = []\n",
        "for ix, row in testingDataFinal.iterrows():\n",
        "  text = row['cleansed_text']\n",
        "  if isNotNan(text) and text!='' and text!=\" \" and text!=None:\n",
        "    #print(text)\n",
        "    tweet = row['cleansed_tweet']\n",
        "    label = 0\n",
        "    text_seq = [char2idx[i] for i in list(text)]\n",
        "    tweet_seq = []\n",
        "    if tweet == '':\n",
        "      tweet_seq.append(word2idx[tweet])\n",
        "    else:\n",
        "      for i in tweet.split():\n",
        "        try:\n",
        "          tweet_seq.append(word2idx[i])\n",
        "        except:\n",
        "          most_sim = all_word_model_ft.wv.most_similar(i)[0][0]\n",
        "          tweet_seq.append(word2idx[most_sim])\n",
        "          not_found_word_list.append([i, most_sim])\n",
        "    if (isNotNan(text)):\n",
        "      comp_char_input.append(torch.tensor(text_seq))\n",
        "      comp_word_input.append(torch.tensor(tweet_seq))\n",
        "      comp_output.append(label)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrqMEJc9eB5I",
        "colab_type": "code",
        "outputId": "9d9946b9-db4a-40cc-ae01-b706becf2c11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_comp_char = pad_sequence(comp_char_input, batch_first=True)\n",
        "x_comp_word = pad_sequence(comp_word_input, batch_first=True)\n",
        "y_comp = torch.tensor(comp_output)\n",
        "print(x_comp_char.shape, x_comp_word.shape,y_comp.shape)\n",
        "\n",
        "class_preds = []\n",
        "comp_data = TensorDataset(x_comp_char, x_comp_word, y_comp)\n",
        "comp_sampler = SequentialSampler(comp_data)\n",
        "comp_dataloader = DataLoader(comp_data, sampler=comp_sampler, batch_size=128)\n",
        "\n",
        "for batch in comp_dataloader:\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      x_comp_char, x_comp_word, y_comp = batch\n",
        "      x_comp_char = x_comp_char.T\n",
        "      x_comp_word = x_comp_word.T\n",
        "  \n",
        "      with torch.no_grad():\n",
        "        outputs = lstm(x_comp_char, x_comp_word)\n",
        "        #eval_loss = loss_fct_classification(outputs, y_comp)\n",
        "\n",
        "      classification_logits = outputs.detach().cpu().numpy()\n",
        "      class_preds.extend(list(np.argmax(classification_logits,axis=1)))\n",
        "      #print(len(class_preds))\n",
        "\n",
        "hgru = []\n",
        "for i in range(len(class_preds)):\n",
        "  label = numbertolabel(class_preds[i])\n",
        "  hgru.append(label)\n",
        "\n",
        "t = 0\n",
        "hgru_predicted = []\n",
        "for i in range(len(comp)):\n",
        "    getextract = comp.iloc[i,3]\n",
        "    if isNotNan(getextract):\n",
        "      hgru_predicted.append(hgru[t])\n",
        "      t = t + 1\n",
        "    else:\n",
        "      hgru_predicted.append(\"\")\n",
        "\n",
        "comp['hgru_meddra_code'] = hgru_predicted"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([201, 113]) torch.Size([201, 31]) torch.Size([201])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BedDeTsQhxcb",
        "colab_type": "code",
        "outputId": "98de5561-c4b0-47b4-f3f0-4fd170c1a09e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 949
        }
      },
      "source": [
        "comp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>extract</th>\n",
              "      <th>type</th>\n",
              "      <th>tweet</th>\n",
              "      <th>cleaned_tweet</th>\n",
              "      <th>meddra_code_cosine</th>\n",
              "      <th>logistic_meddra_code</th>\n",
              "      <th>svc_meddra_code</th>\n",
              "      <th>cleansed_text</th>\n",
              "      <th>cleansed_tweet</th>\n",
              "      <th>hgru_meddra_code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>332317478170546176</td>\n",
              "      <td>27.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>allergies</td>\n",
              "      <td>ADR</td>\n",
              "      <td>do you have any medication allergies? \"asthma!...</td>\n",
              "      <td>do you have any medication allergies ? asthma ...</td>\n",
              "      <td>10001718</td>\n",
              "      <td>10013661</td>\n",
              "      <td>10013661</td>\n",
              "      <td>allergies</td>\n",
              "      <td>do you have any medication allergies  asthma  ...</td>\n",
              "      <td>10001718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>347806215776116737</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>hurt your liver</td>\n",
              "      <td>ADR</td>\n",
              "      <td>@ashleylvivian if #avelox has hurt your liver,...</td>\n",
              "      <td>if a velox has hurt your liver , avoid tylenol...</td>\n",
              "      <td>10024668</td>\n",
              "      <td>10024668</td>\n",
              "      <td>10024668</td>\n",
              "      <td>hurt your liver</td>\n",
              "      <td>if a velox has hurt your liver , avoid tylenol...</td>\n",
              "      <td>10024668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>350336129817509888</td>\n",
              "      <td>75.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>length of focus today about 30 seconds</td>\n",
              "      <td>ADR</td>\n",
              "      <td>apparently, baclofen greatly exacerbates the \"...</td>\n",
              "      <td>apparently , baclofen greatly exacerbates the ...</td>\n",
              "      <td>10016356</td>\n",
              "      <td>10047896</td>\n",
              "      <td>10033557</td>\n",
              "      <td>length of focus today about 30 seconds</td>\n",
              "      <td>apparently , baclofen greatly exacerbates the ...</td>\n",
              "      <td>10047900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>348644040444637184</td>\n",
              "      <td>26.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>dreams</td>\n",
              "      <td>ADR</td>\n",
              "      <td>@bilgeebiri oh robitussin dreams are notorious...</td>\n",
              "      <td>oh robitussin dreams are notorious although wh...</td>\n",
              "      <td>10079069</td>\n",
              "      <td>10000125</td>\n",
              "      <td>10000125</td>\n",
              "      <td>dreams</td>\n",
              "      <td>oh robitussin dreams are notorious although wh...</td>\n",
              "      <td>10000125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>352217412046823425</td>\n",
              "      <td>84.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>tendon rupture</td>\n",
              "      <td>ADR</td>\n",
              "      <td>#eds friends! anybody taken #cipro? (antibioti...</td>\n",
              "      <td>eds friends ! anybody taken cipro ? antibiotic...</td>\n",
              "      <td>10043248</td>\n",
              "      <td>10043248</td>\n",
              "      <td>10043248</td>\n",
              "      <td>tendon rupture</td>\n",
              "      <td>eds friends  anybody taken cipro  antibiotic c...</td>\n",
              "      <td>10043248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>434</th>\n",
              "      <td>331289838856835072</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>okay trazodone, work your magic.</td>\n",
              "      <td>okay trazodone , work your magic</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>None</td>\n",
              "      <td>okay trazodone , work your magic</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>435</th>\n",
              "      <td>332907977700937731</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>tomorrow, sending mummers her mother's day pos...</td>\n",
              "      <td>tomorrow , sending mummers her mother s day po...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>None</td>\n",
              "      <td>tomorrow , sending mummers her mother s day po...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>436</th>\n",
              "      <td>333123450254274560</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@kyleecakes4 i was given like 5mg of abilify a...</td>\n",
              "      <td>i was given like 5 mg of abilify and i still d...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>None</td>\n",
              "      <td>i was given like 5 mg of abilify and i still d...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>437</th>\n",
              "      <td>341950988455927808</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>stand down, tweeps! stand down! i did it wrong...</td>\n",
              "      <td>stand down , tweeps ! stand down ! i did it wr...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>None</td>\n",
              "      <td>stand down , tweeps  stand down  i did it wron...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>438</th>\n",
              "      <td>350709708484644864</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@loveone_another: i'm lowkey pill junkie. &amp;gt;...</td>\n",
              "      <td>i am lowkey pill junkie annoyed vivance , xana...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>None</td>\n",
              "      <td>i am lowkey pill junkie annoyed vivance , xana...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>439 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               tweet_id  ...  hgru_meddra_code\n",
              "0    332317478170546176  ...          10001718\n",
              "1    347806215776116737  ...          10024668\n",
              "2    350336129817509888  ...          10047900\n",
              "3    348644040444637184  ...          10000125\n",
              "4    352217412046823425  ...          10043248\n",
              "..                  ...  ...               ...\n",
              "434  331289838856835072  ...                  \n",
              "435  332907977700937731  ...                  \n",
              "436  333123450254274560  ...                  \n",
              "437  341950988455927808  ...                  \n",
              "438  350709708484644864  ...                  \n",
              "\n",
              "[439 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v1EhFhA3jwC",
        "colab_type": "text"
      },
      "source": [
        "Get The best - Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L81WBAm73umk",
        "colab_type": "code",
        "outputId": "50e556ab-4183-4bdb-9474-4efeec3288c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        }
      },
      "source": [
        "task3mix"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "      <th>cosine_meddra_code</th>\n",
              "      <th>logistic_meddra_code</th>\n",
              "      <th>svc_meddra_code</th>\n",
              "      <th>hgru_meddra_code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>allergies</td>\n",
              "      <td>10013661</td>\n",
              "      <td>do you have any medication allergies ? asthma ...</td>\n",
              "      <td>10001720</td>\n",
              "      <td>10013661</td>\n",
              "      <td>10013661</td>\n",
              "      <td>10001718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hurt your liver</td>\n",
              "      <td>10024668</td>\n",
              "      <td>if a velox has hurt your liver , avoid tylenol...</td>\n",
              "      <td>10024668</td>\n",
              "      <td>10024668</td>\n",
              "      <td>10024668</td>\n",
              "      <td>10001718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ad</td>\n",
              "      <td>10003731</td>\n",
              "      <td>apparently , baclofen greatly exacerbates the ...</td>\n",
              "      <td>10074073</td>\n",
              "      <td>10011469</td>\n",
              "      <td>10011469</td>\n",
              "      <td>10001718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>focus</td>\n",
              "      <td>10003738</td>\n",
              "      <td>apparently , baclofen greatly exacerbates the ...</td>\n",
              "      <td>10012792</td>\n",
              "      <td>10003738</td>\n",
              "      <td>10003738</td>\n",
              "      <td>10001718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>died</td>\n",
              "      <td>10011906</td>\n",
              "      <td>pt of mine died from cipro rt if only more doc...</td>\n",
              "      <td>10068111</td>\n",
              "      <td>10068111</td>\n",
              "      <td>10068111</td>\n",
              "      <td>10079024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>360</th>\n",
              "      <td>hypochondria</td>\n",
              "      <td>10020965</td>\n",
              "      <td>hmm , interesting i may have been undergoing b...</td>\n",
              "      <td>10020965</td>\n",
              "      <td>10020965</td>\n",
              "      <td>10020965</td>\n",
              "      <td>10065032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>361</th>\n",
              "      <td>withdrawal</td>\n",
              "      <td>10048010</td>\n",
              "      <td>hmm , interesting i may have been undergoing b...</td>\n",
              "      <td>10073281</td>\n",
              "      <td>10073281</td>\n",
              "      <td>10073281</td>\n",
              "      <td>10044034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>362</th>\n",
              "      <td>low</td>\n",
              "      <td>10024919</td>\n",
              "      <td>i hate the low after the high on vyvanse</td>\n",
              "      <td>10024919</td>\n",
              "      <td>10024895</td>\n",
              "      <td>10024891</td>\n",
              "      <td>10028322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>363</th>\n",
              "      <td>weight gain</td>\n",
              "      <td>10047896</td>\n",
              "      <td>on a related note tysabri causes weight gain a...</td>\n",
              "      <td>10047896</td>\n",
              "      <td>10047896</td>\n",
              "      <td>10047896</td>\n",
              "      <td>10002855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>364</th>\n",
              "      <td>brain zap</td>\n",
              "      <td>10014358</td>\n",
              "      <td>cymbalta withdrawal has reached a peak , lost ...</td>\n",
              "      <td>10006120</td>\n",
              "      <td>10033775</td>\n",
              "      <td>10033775</td>\n",
              "      <td>10021030</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>365 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                text     label  ... svc_meddra_code  hgru_meddra_code\n",
              "0          allergies  10013661  ...        10013661          10001718\n",
              "1    hurt your liver  10024668  ...        10024668          10001718\n",
              "2                 ad  10003731  ...        10011469          10001718\n",
              "3              focus  10003738  ...        10003738          10001718\n",
              "4               died  10011906  ...        10068111          10079024\n",
              "..               ...       ...  ...             ...               ...\n",
              "360     hypochondria  10020965  ...        10020965          10065032\n",
              "361       withdrawal  10048010  ...        10073281          10044034\n",
              "362              low  10024919  ...        10024891          10028322\n",
              "363      weight gain  10047896  ...        10047896          10002855\n",
              "364        brain zap  10014358  ...        10033775          10021030\n",
              "\n",
              "[365 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SX7sNxcOA-ke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "def getacc(original,predicted):\n",
        "  return accuracy_score(original, predicted)\n",
        "\n",
        "def getF1(original,predicted):\n",
        "  return f1_score(original, predicted, average='macro')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-pOvy3A8VlC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use only four\n",
        "#def decidefinal(cosine_label,logistic_label,svc_label,hgru_label):\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jn6LWzSLFGe",
        "colab_type": "code",
        "outputId": "5f59deb9-c44e-42a0-c051-7898998e1d92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_test = len(list(task3mix['label']))\n",
        "print((y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "365\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2oypscwMXec",
        "colab_type": "code",
        "outputId": "28bbc461-6a8e-4649-c1ec-6ef64ddea728",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "types = [\"cosine\",\"logistic\",\"svc\",\"hgru\",\"mix\",\"mix1\"]\n",
        "finalpred = []\n",
        "original = list(task3mix['label'])\n",
        "cosinepreds = task3mix['cosine_meddra_code']\n",
        "logisticpreds = task3mix['logistic_meddra_code']\n",
        "svcpreds = task3mix['svc_meddra_code']\n",
        "hgrupreds = task3mix['svc_meddra_code']\n",
        "\n",
        "\n",
        "for t in range(len(types)):\n",
        "  typex = types[t]\n",
        "  if typex == \"mix\":\n",
        "    finalpred = []\n",
        "    for i in range((y_test)):  \n",
        "      preds = [cosinepreds[i],            \n",
        "               logisticpreds[i],            \n",
        "               svcpreds[i],             \n",
        "               hgrupreds[i]]   \n",
        "      finalpred.append(Counter(preds).most_common(1)[0][0])\n",
        "    acc = getacc(original,finalpred)\n",
        "    f1 = getF1(original,finalpred)\n",
        "    print(\"For type:\",typex,\"Acc:\",acc,\"F Score:\",f1)\n",
        "  if typex == \"mix1\":\n",
        "    finalpred = []\n",
        "    for i in range((y_test)):  \n",
        "      preds = [cosinepreds[i],            \n",
        "               logisticpreds[i], logisticpreds[i],           \n",
        "               svcpreds[i], svcpreds[i],     \n",
        "               hgrupreds[i]]   \n",
        "      finalpred.append(Counter(preds).most_common(1)[0][0])\n",
        "    acc = getacc(original,finalpred)\n",
        "    f1 = getF1(original,finalpred)\n",
        "    print(\"For type:\",typex,\"Acc:\",acc,\"F Score:\",f1)\n",
        "  if typex == \"cosine\":\n",
        "    finalpred = []\n",
        "    for i in range((y_test)):  \n",
        "      preds = [cosinepreds[i],cosinepreds[i],cosinepreds[i],            \n",
        "               logisticpreds[i],            \n",
        "               svcpreds[i],             \n",
        "               hgrupreds[i]]   \n",
        "      finalpred.append(Counter(preds).most_common(1)[0][0])\n",
        "    acc = getacc(original,finalpred)\n",
        "    f1 = getF1(original,finalpred)\n",
        "    print(\"For type:\",typex,\"Acc:\",acc,\"F Score:\",f1)\n",
        "  if typex == \"logistic\":\n",
        "    finalpred = []\n",
        "    for i in range((y_test)):  \n",
        "      preds = [cosinepreds[i],            \n",
        "               logisticpreds[i],logisticpreds[i],logisticpreds[i],            \n",
        "               svcpreds[i],             \n",
        "               hgrupreds[i]]   \n",
        "      finalpred.append(Counter(preds).most_common(1)[0][0])\n",
        "    acc = getacc(original,finalpred)\n",
        "    f1 = getF1(original,finalpred)\n",
        "    print(\"For type:\",typex,\"Acc:\",acc,\"F Score:\",f1)\n",
        "  if typex == \"svc\":\n",
        "    finalpred = []\n",
        "    for i in range((y_test)):  \n",
        "      preds = [cosinepreds[i],          \n",
        "               logisticpreds[i],            \n",
        "               svcpreds[i], svcpreds[i],svcpreds[i],             \n",
        "               hgrupreds[i]]   \n",
        "      finalpred.append(Counter(preds).most_common(1)[0][0])\n",
        "    acc = getacc(original,finalpred)\n",
        "    f1 = getF1(original,finalpred)\n",
        "    print(\"For type:\",typex,\"Acc:\",acc,\"F Score:\",f1)\n",
        "  if typex == \"hgru\":\n",
        "    finalpred = []\n",
        "    for i in range((y_test)):  \n",
        "      preds = [cosinepreds[i],          \n",
        "               logisticpreds[i],            \n",
        "               svcpreds[i],             \n",
        "               hgrupreds[i],hgrupreds[i],hgrupreds[i]]   \n",
        "      finalpred.append(Counter(preds).most_common(1)[0][0])\n",
        "    acc = getacc(original,finalpred)\n",
        "    f1 = getF1(original,finalpred)\n",
        "    print(\"For type:\",typex,\"Acc:\",acc,\"F Score:\",f1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For type: cosine Acc: 0.4547945205479452 F Score: 0.3893429117361379\n",
            "For type: logistic Acc: 0.4712328767123288 F Score: 0.35153161015229983\n",
            "For type: svc Acc: 0.4575342465753425 F Score: 0.32305062260758466\n",
            "For type: hgru Acc: 0.4575342465753425 F Score: 0.32305062260758466\n",
            "For type: mix Acc: 0.4657534246575342 F Score: 0.33730284951803935\n",
            "For type: mix1 Acc: 0.4657534246575342 F Score: 0.33730284951803935\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc4M4ssHOYXL",
        "colab_type": "code",
        "outputId": "6e443dcd-a61a-4d0f-f80c-813cfe64c84b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "comp_test = len(list(comp['tweet_id']))\n",
        "print((comp_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "439\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DMGwj2TiGRQ",
        "colab_type": "code",
        "outputId": "22d65eb1-b178-49d5-ba92-b1efb3d4de2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 949
        }
      },
      "source": [
        "comp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>extract</th>\n",
              "      <th>type</th>\n",
              "      <th>tweet</th>\n",
              "      <th>cleaned_tweet</th>\n",
              "      <th>meddra_code_cosine</th>\n",
              "      <th>logistic_meddra_code</th>\n",
              "      <th>svc_meddra_code</th>\n",
              "      <th>cleansed_text</th>\n",
              "      <th>cleansed_tweet</th>\n",
              "      <th>hgru_meddra_code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>332317478170546176</td>\n",
              "      <td>27.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>allergies</td>\n",
              "      <td>ADR</td>\n",
              "      <td>do you have any medication allergies? \"asthma!...</td>\n",
              "      <td>do you have any medication allergies ? asthma ...</td>\n",
              "      <td>10001718</td>\n",
              "      <td>10013661</td>\n",
              "      <td>10013661</td>\n",
              "      <td>allergies</td>\n",
              "      <td>do you have any medication allergies  asthma  ...</td>\n",
              "      <td>10001718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>347806215776116737</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>hurt your liver</td>\n",
              "      <td>ADR</td>\n",
              "      <td>@ashleylvivian if #avelox has hurt your liver,...</td>\n",
              "      <td>if a velox has hurt your liver , avoid tylenol...</td>\n",
              "      <td>10024668</td>\n",
              "      <td>10024668</td>\n",
              "      <td>10024668</td>\n",
              "      <td>hurt your liver</td>\n",
              "      <td>if a velox has hurt your liver , avoid tylenol...</td>\n",
              "      <td>10024668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>350336129817509888</td>\n",
              "      <td>75.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>length of focus today about 30 seconds</td>\n",
              "      <td>ADR</td>\n",
              "      <td>apparently, baclofen greatly exacerbates the \"...</td>\n",
              "      <td>apparently , baclofen greatly exacerbates the ...</td>\n",
              "      <td>10016356</td>\n",
              "      <td>10047896</td>\n",
              "      <td>10033557</td>\n",
              "      <td>length of focus today about 30 seconds</td>\n",
              "      <td>apparently , baclofen greatly exacerbates the ...</td>\n",
              "      <td>10047900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>348644040444637184</td>\n",
              "      <td>26.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>dreams</td>\n",
              "      <td>ADR</td>\n",
              "      <td>@bilgeebiri oh robitussin dreams are notorious...</td>\n",
              "      <td>oh robitussin dreams are notorious although wh...</td>\n",
              "      <td>10079069</td>\n",
              "      <td>10000125</td>\n",
              "      <td>10000125</td>\n",
              "      <td>dreams</td>\n",
              "      <td>oh robitussin dreams are notorious although wh...</td>\n",
              "      <td>10000125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>352217412046823425</td>\n",
              "      <td>84.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>tendon rupture</td>\n",
              "      <td>ADR</td>\n",
              "      <td>#eds friends! anybody taken #cipro? (antibioti...</td>\n",
              "      <td>eds friends ! anybody taken cipro ? antibiotic...</td>\n",
              "      <td>10043248</td>\n",
              "      <td>10043248</td>\n",
              "      <td>10043248</td>\n",
              "      <td>tendon rupture</td>\n",
              "      <td>eds friends  anybody taken cipro  antibiotic c...</td>\n",
              "      <td>10043248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>434</th>\n",
              "      <td>331289838856835072</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>okay trazodone, work your magic.</td>\n",
              "      <td>okay trazodone , work your magic</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>None</td>\n",
              "      <td>okay trazodone , work your magic</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>435</th>\n",
              "      <td>332907977700937731</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>tomorrow, sending mummers her mother's day pos...</td>\n",
              "      <td>tomorrow , sending mummers her mother s day po...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>None</td>\n",
              "      <td>tomorrow , sending mummers her mother s day po...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>436</th>\n",
              "      <td>333123450254274560</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@kyleecakes4 i was given like 5mg of abilify a...</td>\n",
              "      <td>i was given like 5 mg of abilify and i still d...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>None</td>\n",
              "      <td>i was given like 5 mg of abilify and i still d...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>437</th>\n",
              "      <td>341950988455927808</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>stand down, tweeps! stand down! i did it wrong...</td>\n",
              "      <td>stand down , tweeps ! stand down ! i did it wr...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>None</td>\n",
              "      <td>stand down , tweeps  stand down  i did it wron...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>438</th>\n",
              "      <td>350709708484644864</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@loveone_another: i'm lowkey pill junkie. &amp;gt;...</td>\n",
              "      <td>i am lowkey pill junkie annoyed vivance , xana...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>None</td>\n",
              "      <td>i am lowkey pill junkie annoyed vivance , xana...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>439 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               tweet_id  ...  hgru_meddra_code\n",
              "0    332317478170546176  ...          10001718\n",
              "1    347806215776116737  ...          10024668\n",
              "2    350336129817509888  ...          10047900\n",
              "3    348644040444637184  ...          10000125\n",
              "4    352217412046823425  ...          10043248\n",
              "..                  ...  ...               ...\n",
              "434  331289838856835072  ...                  \n",
              "435  332907977700937731  ...                  \n",
              "436  333123450254274560  ...                  \n",
              "437  341950988455927808  ...                  \n",
              "438  350709708484644864  ...                  \n",
              "\n",
              "[439 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pw__Er-2VJlQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "comp = comp.drop(['cleaned_tweet', 'cleansed_tweet', 'cleansed_text'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNRR3SIhkiml",
        "colab_type": "code",
        "outputId": "0f91565c-28ae-456a-cc9a-2c98c05d6fbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 847
        }
      },
      "source": [
        "comp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>extract</th>\n",
              "      <th>type</th>\n",
              "      <th>tweet</th>\n",
              "      <th>meddra_code_cosine</th>\n",
              "      <th>logistic_meddra_code</th>\n",
              "      <th>svc_meddra_code</th>\n",
              "      <th>hgru_meddra_code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>332317478170546176</td>\n",
              "      <td>27.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>allergies</td>\n",
              "      <td>ADR</td>\n",
              "      <td>do you have any medication allergies? \"asthma!...</td>\n",
              "      <td>10001718</td>\n",
              "      <td>10013661</td>\n",
              "      <td>10013661</td>\n",
              "      <td>10001718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>347806215776116737</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>hurt your liver</td>\n",
              "      <td>ADR</td>\n",
              "      <td>@ashleylvivian if #avelox has hurt your liver,...</td>\n",
              "      <td>10024668</td>\n",
              "      <td>10024668</td>\n",
              "      <td>10024668</td>\n",
              "      <td>10024668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>350336129817509888</td>\n",
              "      <td>75.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>length of focus today about 30 seconds</td>\n",
              "      <td>ADR</td>\n",
              "      <td>apparently, baclofen greatly exacerbates the \"...</td>\n",
              "      <td>10016356</td>\n",
              "      <td>10047896</td>\n",
              "      <td>10033557</td>\n",
              "      <td>10047900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>348644040444637184</td>\n",
              "      <td>26.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>dreams</td>\n",
              "      <td>ADR</td>\n",
              "      <td>@bilgeebiri oh robitussin dreams are notorious...</td>\n",
              "      <td>10079069</td>\n",
              "      <td>10000125</td>\n",
              "      <td>10000125</td>\n",
              "      <td>10000125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>352217412046823425</td>\n",
              "      <td>84.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>tendon rupture</td>\n",
              "      <td>ADR</td>\n",
              "      <td>#eds friends! anybody taken #cipro? (antibioti...</td>\n",
              "      <td>10043248</td>\n",
              "      <td>10043248</td>\n",
              "      <td>10043248</td>\n",
              "      <td>10043248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>434</th>\n",
              "      <td>331289838856835072</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>okay trazodone, work your magic.</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>435</th>\n",
              "      <td>332907977700937731</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>tomorrow, sending mummers her mother's day pos...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>436</th>\n",
              "      <td>333123450254274560</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@kyleecakes4 i was given like 5mg of abilify a...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>437</th>\n",
              "      <td>341950988455927808</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>stand down, tweeps! stand down! i did it wrong...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>438</th>\n",
              "      <td>350709708484644864</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@loveone_another: i'm lowkey pill junkie. &amp;gt;...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>439 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               tweet_id  start  ...  svc_meddra_code hgru_meddra_code\n",
              "0    332317478170546176   27.0  ...         10013661         10001718\n",
              "1    347806215776116737   30.0  ...         10024668         10024668\n",
              "2    350336129817509888   75.0  ...         10033557         10047900\n",
              "3    348644040444637184   26.0  ...         10000125         10000125\n",
              "4    352217412046823425   84.0  ...         10043248         10043248\n",
              "..                  ...    ...  ...              ...              ...\n",
              "434  331289838856835072    NaN  ...                                  \n",
              "435  332907977700937731    NaN  ...                                  \n",
              "436  333123450254274560    NaN  ...                                  \n",
              "437  341950988455927808    NaN  ...                                  \n",
              "438  350709708484644864    NaN  ...                                  \n",
              "\n",
              "[439 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHl7KN1TjTr3",
        "colab_type": "code",
        "outputId": "6c632692-b513-4d4a-bdbc-2e0dae9c6ec4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "finalmeddracode = []\n",
        "for i in range(len(comp)):\n",
        "  ex = comp.iloc[i,3]\n",
        "  cs = comp.iloc[i,6]\n",
        "  #print(cs)\n",
        "  if isNotNan(ex):\n",
        "    finalmeddracode.append(cs)\n",
        "  else:\n",
        "    finalmeddracode.append(\"\")\n",
        "\n",
        "print(len(finalmeddracode))\n",
        "comp['meddra_code'] = finalmeddracode"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "439\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfSGYhckkaxM",
        "colab_type": "code",
        "outputId": "d5fcf2a9-bc8d-4511-8510-b09131d26871",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 847
        }
      },
      "source": [
        "comp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>extract</th>\n",
              "      <th>type</th>\n",
              "      <th>tweet</th>\n",
              "      <th>meddra_code_cosine</th>\n",
              "      <th>logistic_meddra_code</th>\n",
              "      <th>svc_meddra_code</th>\n",
              "      <th>hgru_meddra_code</th>\n",
              "      <th>meddra_code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>332317478170546176</td>\n",
              "      <td>27.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>allergies</td>\n",
              "      <td>ADR</td>\n",
              "      <td>do you have any medication allergies? \"asthma!...</td>\n",
              "      <td>10001718</td>\n",
              "      <td>10013661</td>\n",
              "      <td>10013661</td>\n",
              "      <td>10001718</td>\n",
              "      <td>10001718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>347806215776116737</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>hurt your liver</td>\n",
              "      <td>ADR</td>\n",
              "      <td>@ashleylvivian if #avelox has hurt your liver,...</td>\n",
              "      <td>10024668</td>\n",
              "      <td>10024668</td>\n",
              "      <td>10024668</td>\n",
              "      <td>10024668</td>\n",
              "      <td>10024668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>350336129817509888</td>\n",
              "      <td>75.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>length of focus today about 30 seconds</td>\n",
              "      <td>ADR</td>\n",
              "      <td>apparently, baclofen greatly exacerbates the \"...</td>\n",
              "      <td>10016356</td>\n",
              "      <td>10047896</td>\n",
              "      <td>10033557</td>\n",
              "      <td>10047900</td>\n",
              "      <td>10016356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>348644040444637184</td>\n",
              "      <td>26.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>dreams</td>\n",
              "      <td>ADR</td>\n",
              "      <td>@bilgeebiri oh robitussin dreams are notorious...</td>\n",
              "      <td>10079069</td>\n",
              "      <td>10000125</td>\n",
              "      <td>10000125</td>\n",
              "      <td>10000125</td>\n",
              "      <td>10079069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>352217412046823425</td>\n",
              "      <td>84.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>tendon rupture</td>\n",
              "      <td>ADR</td>\n",
              "      <td>#eds friends! anybody taken #cipro? (antibioti...</td>\n",
              "      <td>10043248</td>\n",
              "      <td>10043248</td>\n",
              "      <td>10043248</td>\n",
              "      <td>10043248</td>\n",
              "      <td>10043248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>434</th>\n",
              "      <td>331289838856835072</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>okay trazodone, work your magic.</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>435</th>\n",
              "      <td>332907977700937731</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>tomorrow, sending mummers her mother's day pos...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>436</th>\n",
              "      <td>333123450254274560</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@kyleecakes4 i was given like 5mg of abilify a...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>437</th>\n",
              "      <td>341950988455927808</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>stand down, tweeps! stand down! i did it wrong...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>438</th>\n",
              "      <td>350709708484644864</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@loveone_another: i'm lowkey pill junkie. &amp;gt;...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>439 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               tweet_id  start  ...  hgru_meddra_code meddra_code\n",
              "0    332317478170546176   27.0  ...          10001718    10001718\n",
              "1    347806215776116737   30.0  ...          10024668    10024668\n",
              "2    350336129817509888   75.0  ...          10047900    10016356\n",
              "3    348644040444637184   26.0  ...          10000125    10079069\n",
              "4    352217412046823425   84.0  ...          10043248    10043248\n",
              "..                  ...    ...  ...               ...         ...\n",
              "434  331289838856835072    NaN  ...                              \n",
              "435  332907977700937731    NaN  ...                              \n",
              "436  333123450254274560    NaN  ...                              \n",
              "437  341950988455927808    NaN  ...                              \n",
              "438  350709708484644864    NaN  ...                              \n",
              "\n",
              "[439 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SphIfxdkk7ar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "comp = comp.drop(['tweet','meddra_code_cosine', 'logistic_meddra_code', 'svc_meddra_code','hgru_meddra_code'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrvhrQ-elHLo",
        "colab_type": "code",
        "outputId": "fbcf5b4d-2c51-46b2-a6c7-6bb27e11bf76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "comp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>extract</th>\n",
              "      <th>type</th>\n",
              "      <th>meddra_code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>332317478170546176</td>\n",
              "      <td>27.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>allergies</td>\n",
              "      <td>ADR</td>\n",
              "      <td>10001718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>347806215776116737</td>\n",
              "      <td>30.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>hurt your liver</td>\n",
              "      <td>ADR</td>\n",
              "      <td>10024668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>350336129817509888</td>\n",
              "      <td>75.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>length of focus today about 30 seconds</td>\n",
              "      <td>ADR</td>\n",
              "      <td>10016356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>348644040444637184</td>\n",
              "      <td>26.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>dreams</td>\n",
              "      <td>ADR</td>\n",
              "      <td>10079069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>352217412046823425</td>\n",
              "      <td>84.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>tendon rupture</td>\n",
              "      <td>ADR</td>\n",
              "      <td>10043248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>434</th>\n",
              "      <td>331289838856835072</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>435</th>\n",
              "      <td>332907977700937731</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>436</th>\n",
              "      <td>333123450254274560</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>437</th>\n",
              "      <td>341950988455927808</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>438</th>\n",
              "      <td>350709708484644864</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>439 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               tweet_id  start  ...  type meddra_code\n",
              "0    332317478170546176   27.0  ...   ADR    10001718\n",
              "1    347806215776116737   30.0  ...   ADR    10024668\n",
              "2    350336129817509888   75.0  ...   ADR    10016356\n",
              "3    348644040444637184   26.0  ...   ADR    10079069\n",
              "4    352217412046823425   84.0  ...   ADR    10043248\n",
              "..                  ...    ...  ...   ...         ...\n",
              "434  331289838856835072    NaN  ...   NaN            \n",
              "435  332907977700937731    NaN  ...   NaN            \n",
              "436  333123450254274560    NaN  ...   NaN            \n",
              "437  341950988455927808    NaN  ...   NaN            \n",
              "438  350709708484644864    NaN  ...   NaN            \n",
              "\n",
              "[439 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obYonyIjlJWU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "comp.to_csv('./prediction_task3.tsv', sep='\\t', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_rGUF2HySz5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "comp.columns = ['tweet_id', 'begin', 'end',  'extraction','type', 'meddra_code']\n",
        "comp = comp[['tweet_id', 'begin', 'end',  'type','extraction', 'meddra_code']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-S7lDJWyweX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "comp.to_csv(\"prediction_task3.tsv\",sep=\"\\t\",index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}